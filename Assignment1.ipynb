{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f375d8d",
   "metadata": {},
   "source": [
    "# Nyrah Balabanian\n",
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e3f66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages for data preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#import packages for tokenization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f5fab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# Open the file in read mode\n",
    "with open('SMSSpamCollection', 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    for line in file:\n",
    "        # Parse the line (by tab)\n",
    "        parsed_line = line.strip().split('\t')\n",
    "        \n",
    "        # Add the parsed data to the list\n",
    "        data.append(parsed_line)\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df_raw = pd.DataFrame(data, columns=['label', 'text']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d193058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f589f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_raw['label'].unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81c74889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the unique values to 0 and 1\n",
    "mapping = {'ham': 0, 'spam': 1}\n",
    "df['label'] = df['label'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45a21720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs\n",
    "df['text'] = df['text'].str.replace(r'http[s]?://\\S+', '', regex=True)\n",
    "\n",
    "# Remove punctuation and numbers\n",
    "df['text'] = df['text'].str.replace(r\"[^a-zA-Z' ]\", '', regex=True)\n",
    "\n",
    "# Convert text to lower case\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c167dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\scoop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\scoop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a2f5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in stemmed_tokens if word not in stop_words]\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dfc8c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['label']\n",
    "X=df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b5f56ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scoop\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_text, preprocessor=lambda x: x, lowercase=False)\n",
    "\n",
    "# Fit the vectorizer to the training data\n",
    "X_tfidf = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ed61f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 7141)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33807c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65798be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.todense()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b94d496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize data first before splitting the data\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_tfidf, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Splitting the Training set further into Training and Validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size = 0.2, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74240bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Number of Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Training</td>\n",
       "      <td>3567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Validation</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test</td>\n",
       "      <td>1115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dataset  Number of Samples\n",
       "0    Training               3567\n",
       "1  Validation                892\n",
       "2        Test               1115"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data distribution in a table\n",
    "# Calculate the sizes of each split\n",
    "train_size = X_train.shape[0]\n",
    "val_size = X_val.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Dataset': ['Training', 'Validation', 'Test'],\n",
    "    'Number of Samples': [train_size, val_size, test_size]\n",
    "}\n",
    "data_distribution = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "data_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6027f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d569b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a5eab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3336893793070944\n"
     ]
    }
   ],
   "source": [
    "test = X_tfidf.todense()\n",
    "print(test[:1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05b7aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "d = X_tfidf.shape[1]  # Number of features in X\n",
    "w = np.random.randn(d, 1)  # Initialize weights\n",
    "b = 0  # Initialize bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d4ffea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):    \n",
    "    #sigmoid outputs a number between 0 and 1\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7fa1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "def compute_predictions(X, w, b):\n",
    "    # Ensure that w is a dense array for matrix multiplication\n",
    "    w_dense = w if isinstance(w, np.ndarray) else w.toarray()\n",
    "\n",
    "    # Perform sparse matrix multiplication and add bias\n",
    "    z = X.dot(w_dense) + b\n",
    "\n",
    "    # Apply the sigmoid function\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41aff628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, y_hat, w, lambda_reg):\n",
    "    # Convert y to a numpy array if it's a Pandas Series\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "    \n",
    "    n = len(y)\n",
    "    binary_cross_entropy_loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    L2 = binary_cross_entropy_loss + lambda_reg * np.sum(w ** 2) / (2 * n)\n",
    "    return L2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7466e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X_train, y_train, y_hat, w, lambda_reg):\n",
    "    y_train_np = y_train.to_numpy() if isinstance(y_train, (pd.Series, pd.DataFrame)) else y_train\n",
    "    y_hat = y_hat.flatten() if y_hat.ndim > 1 else y_hat\n",
    "    n = len(y_train_np)\n",
    "\n",
    "    # For sparse matrix multiplication, use the dot method of the sparse matrix\n",
    "    dw = -(X_train.T.dot(y_train_np - y_hat)) / n \n",
    "\n",
    "    # Flatten w for the addition operation\n",
    "    w_flattened = w.flatten() if w.ndim > 1 else w\n",
    "\n",
    "    # Adding the regularization term\n",
    "    dw += lambda_reg * w_flattened / n\n",
    "\n",
    "    db = -np.mean(y_train_np - y_hat)\n",
    "\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c7c2312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    true_positives = sum((y_pred == 1) & (y_true == 1))\n",
    "    true_negatives = sum((y_pred == 0) & (y_true == 0))\n",
    "    false_positives = sum((y_pred == 1) & (y_true == 0))\n",
    "    false_negatives = sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "    accuracy = (true_positives + true_negatives) / len(y_true)\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bca638be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini batch gradient descent\n",
    "def mini_batch(X_train, y_train,lambda_reg):\n",
    "    eta = 0.1  # Learning rate\n",
    "    n_epochs = 1000\n",
    "    m = X_train.shape[0]  # Number of instances\n",
    "    #lambda_reg = 1  # Regularization parameter\n",
    "    batch_size = 32  # Mini-batch size\n",
    "\n",
    "    # Randomly initialize model parameters\n",
    "    np.random.seed(42)\n",
    "    w = np.random.randn(X_train.shape[1], 1)  # Initialize weights (excluding bias)\n",
    "    b = np.random.randn()  # Initialize bias\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_train_shuffled = X_train[shuffled_indices]\n",
    "        y_train_shuffled = y_train.iloc[shuffled_indices]\n",
    "\n",
    "\n",
    "        for i in range(0, m, batch_size):\n",
    "            xi = X_train_shuffled[i:i+batch_size]\n",
    "            yi = y_train_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Matrix multiplication (ensure xi and w are compatible)\n",
    "            # If w is dense and xi is sparse, you may need to adjust the operation\n",
    "            preds = sigmoid(xi.dot(w) + b)  # Use dot for sparse-dense multiplication\n",
    "\n",
    "            dw, db = compute_gradients(xi, yi, preds, w, lambda_reg)\n",
    "\n",
    "            # Update weights and bias\n",
    "            w -= eta * dw.reshape(w.shape)\n",
    "            b -= eta * db\n",
    "\n",
    "            val_preds = sigmoid(X_val.dot(w) + b).flatten()\n",
    "            val_preds_binary = (val_preds >= 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy, precision, recall, f1 = calculate_metrics(y_val, val_preds_binary)\n",
    "\n",
    "            # Compute loss on validation set\n",
    "            val_loss = compute_loss(y_val, sigmoid(X_val.dot(w) + b), w, lambda_reg)\n",
    "\n",
    "            test_preds = sigmoid(X_test.dot(w) + b).flatten()\n",
    "            test_preds_binary = (test_preds >= 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy1, precision1, recall1, f1_1 = calculate_metrics(y_test, test_preds_binary)\n",
    "\n",
    "            # Compute loss on validation set\n",
    "            test_loss = compute_loss(y_test, sigmoid(X_test.dot(w) + b), w, lambda_reg)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Validation Loss: {val_loss}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Test Loss: {test_loss}, Accuracy: {accuracy1}, Precision: {precision1}, Recall: {recall1}, F1 Score: {f1_1}\")\n",
    "            \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b1ac533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.39292883560295316\n",
      "Epoch 20, Loss: 0.3927423981298147\n",
      "Epoch 40, Loss: 0.3966798034430748\n",
      "Epoch 60, Loss: 0.39081547925175697\n",
      "Epoch 80, Loss: 0.39159573829601896\n"
     ]
    }
   ],
   "source": [
    "# Stochastic gradient descent\n",
    "n_epochs = 1000  # Number of epochs\n",
    "m = X_train.shape[0]  # Number of instances\n",
    "lambda_reg = 1  # Regularization parameter\n",
    "\n",
    "# Randomly initialize model parameters\n",
    "np.random.seed(42)\n",
    "w = np.random.randn(X_train.shape[1], 1)  # Initialize weights\n",
    "b = np.random.randn()  # Initialize bias\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_train_shuffled = X_train[shuffled_indices]\n",
    "    y_train_shuffled = y_train.iloc[shuffled_indices]\n",
    "\n",
    "    for i in range(m):\n",
    "        xi = X_train_shuffled[i:i+1]  # Selecting one instance\n",
    "        yi = y_train_shuffled[i:i+1]\n",
    "\n",
    "        # Compute predictions\n",
    "        preds = sigmoid(xi.dot(w) + b)\n",
    "\n",
    "        # Compute gradients\n",
    "        dw, db = compute_gradients(xi, yi, preds, w, lambda_reg)\n",
    "\n",
    "        # Update weights and bias\n",
    "        w -= eta * dw.reshape(w.shape)\n",
    "        b -= eta * db\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {compute_loss(y_train_shuffled, sigmoid(X_train_shuffled.dot(w) + b), w, lambda_reg)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Validation Loss: 0.5009536514732899, Accuracy: 0.8161434977578476, Precision: 0.06896551724137931, Recall: 0.03508771929824561, F1 Score: 0.046511627906976744\n",
      "Epoch 0, Test Loss: 0.5096430340899001, Accuracy: 0.8260089686098655, Precision: 0.17647058823529413, Recall: 0.055900621118012424, F1 Score: 0.08490566037735849\n",
      "Epoch 100, Validation Loss: 0.4631354336943751, Accuracy: 0.8968609865470852, Precision: 0.9230769230769231, Recall: 0.21052631578947367, F1 Score: 0.34285714285714286\n",
      "Epoch 100, Test Loss: 0.514611551776682, Accuracy: 0.9022421524663677, Precision: 0.9333333333333333, Recall: 0.34782608695652173, F1 Score: 0.5067873303167421\n",
      "Epoch 200, Validation Loss: 0.47798501820272005, Accuracy: 0.9282511210762332, Precision: 0.9807692307692307, Recall: 0.4473684210526316, F1 Score: 0.6144578313253012\n",
      "Epoch 200, Test Loss: 0.5419545920071235, Accuracy: 0.9318385650224216, Precision: 0.956989247311828, Recall: 0.5527950310559007, F1 Score: 0.7007874015748031\n",
      "Epoch 300, Validation Loss: 0.4849750751496315, Accuracy: 0.9327354260089686, Precision: 0.9821428571428571, Recall: 0.4824561403508772, F1 Score: 0.6470588235294117\n",
      "Epoch 300, Test Loss: 0.5523671564347539, Accuracy: 0.9399103139013453, Precision: 0.9607843137254902, Recall: 0.6086956521739131, F1 Score: 0.7452471482889734\n",
      "Epoch 400, Validation Loss: 0.4878099764920347, Accuracy: 0.9360986547085202, Precision: 0.9830508474576272, Recall: 0.5087719298245614, F1 Score: 0.6705202312138728\n",
      "Epoch 400, Test Loss: 0.5557197824512363, Accuracy: 0.9452914798206278, Precision: 0.9716981132075472, Recall: 0.639751552795031, F1 Score: 0.7715355805243446\n",
      "Epoch 500, Validation Loss: 0.4888559078143677, Accuracy: 0.9394618834080718, Precision: 0.9838709677419355, Recall: 0.5350877192982456, F1 Score: 0.6931818181818181\n",
      "Epoch 500, Test Loss: 0.5565117034393423, Accuracy: 0.9488789237668162, Precision: 0.9727272727272728, Recall: 0.6645962732919255, F1 Score: 0.7896678966789669\n",
      "Epoch 600, Validation Loss: 0.4894401251275989, Accuracy: 0.9405829596412556, Precision: 0.9841269841269841, Recall: 0.543859649122807, F1 Score: 0.7005649717514124\n",
      "Epoch 600, Test Loss: 0.5566334711770672, Accuracy: 0.9479820627802691, Precision: 0.9813084112149533, Recall: 0.6521739130434783, F1 Score: 0.7835820895522387\n",
      "Epoch 700, Validation Loss: 0.489640231055388, Accuracy: 0.9372197309417041, Precision: 0.9833333333333333, Recall: 0.5175438596491229, F1 Score: 0.6781609195402298\n",
      "Epoch 700, Test Loss: 0.556419543545011, Accuracy: 0.9479820627802691, Precision: 0.9813084112149533, Recall: 0.6521739130434783, F1 Score: 0.7835820895522387\n",
      "Epoch 800, Validation Loss: 0.48975898678327695, Accuracy: 0.9372197309417041, Precision: 0.9833333333333333, Recall: 0.5175438596491229, F1 Score: 0.6781609195402298\n",
      "Epoch 800, Test Loss: 0.5562104863915978, Accuracy: 0.947085201793722, Precision: 0.9811320754716981, Recall: 0.6459627329192547, F1 Score: 0.7790262172284644\n",
      "Epoch 900, Validation Loss: 0.48995693396361784, Accuracy: 0.9372197309417041, Precision: 0.9833333333333333, Recall: 0.5175438596491229, F1 Score: 0.6781609195402298\n",
      "Epoch 900, Test Loss: 0.5561822214672855, Accuracy: 0.947085201793722, Precision: 0.9811320754716981, Recall: 0.6459627329192547, F1 Score: 0.7790262172284644\n",
      "Epoch 0, Validation Loss: 0.5006409260645963, Accuracy: 0.8161434977578476, Precision: 0.06896551724137931, Recall: 0.03508771929824561, F1 Score: 0.046511627906976744\n",
      "Epoch 0, Test Loss: 0.5094205067997144, Accuracy: 0.8269058295964126, Precision: 0.18, Recall: 0.055900621118012424, F1 Score: 0.08530805687203792\n",
      "Epoch 100, Validation Loss: 0.46197461867439293, Accuracy: 0.8968609865470852, Precision: 0.9230769230769231, Recall: 0.21052631578947367, F1 Score: 0.34285714285714286\n",
      "Epoch 100, Test Loss: 0.5113226648632405, Accuracy: 0.8932735426008969, Precision: 0.9038461538461539, Recall: 0.2919254658385093, F1 Score: 0.44131455399061026\n",
      "Epoch 200, Validation Loss: 0.4754174915605849, Accuracy: 0.922645739910314, Precision: 0.9591836734693877, Recall: 0.41228070175438597, F1 Score: 0.5766871165644172\n",
      "Epoch 200, Test Loss: 0.5354290555625946, Accuracy: 0.9246636771300448, Precision: 0.9325842696629213, Recall: 0.515527950310559, F1 Score: 0.6639999999999999\n",
      "Epoch 300, Validation Loss: 0.4818695209921626, Accuracy: 0.9316143497757847, Precision: 0.9818181818181818, Recall: 0.47368421052631576, F1 Score: 0.6390532544378698\n",
      "Epoch 300, Test Loss: 0.5448102780501898, Accuracy: 0.9363228699551569, Precision: 0.95, Recall: 0.5900621118012422, F1 Score: 0.7279693486590039\n",
      "Epoch 400, Validation Loss: 0.4845570001826766, Accuracy: 0.9338565022421524, Precision: 0.9824561403508771, Recall: 0.49122807017543857, F1 Score: 0.6549707602339181\n",
      "Epoch 400, Test Loss: 0.5478601504044583, Accuracy: 0.9408071748878923, Precision: 0.9611650485436893, Recall: 0.6149068322981367, F1 Score: 0.7500000000000001\n",
      "Epoch 500, Validation Loss: 0.48570271376739615, Accuracy: 0.9372197309417041, Precision: 0.9833333333333333, Recall: 0.5175438596491229, F1 Score: 0.6781609195402298\n",
      "Epoch 500, Test Loss: 0.548792677676882, Accuracy: 0.9452914798206278, Precision: 0.9716981132075472, Recall: 0.639751552795031, F1 Score: 0.7715355805243446\n",
      "Epoch 600, Validation Loss: 0.48631140206611995, Accuracy: 0.9349775784753364, Precision: 0.9827586206896551, Recall: 0.5, F1 Score: 0.6627906976744186\n",
      "Epoch 600, Test Loss: 0.5490503631240286, Accuracy: 0.9443946188340807, Precision: 0.9714285714285714, Recall: 0.6335403726708074, F1 Score: 0.7669172932330828\n",
      "Epoch 700, Validation Loss: 0.48661028091699154, Accuracy: 0.9372197309417041, Precision: 0.9833333333333333, Recall: 0.5175438596491229, F1 Score: 0.6781609195402298\n",
      "Epoch 700, Test Loss: 0.5489638555968792, Accuracy: 0.9452914798206278, Precision: 0.9716981132075472, Recall: 0.639751552795031, F1 Score: 0.7715355805243446\n",
      "Epoch 800, Validation Loss: 0.48681965143424244, Accuracy: 0.9372197309417041, Precision: 0.9833333333333333, Recall: 0.5175438596491229, F1 Score: 0.6781609195402298\n",
      "Epoch 800, Test Loss: 0.5488657511247288, Accuracy: 0.9443946188340807, Precision: 0.9714285714285714, Recall: 0.6335403726708074, F1 Score: 0.7669172932330828\n",
      "Epoch 900, Validation Loss: 0.4870432725149223, Accuracy: 0.9372197309417041, Precision: 0.9833333333333333, Recall: 0.5175438596491229, F1 Score: 0.6781609195402298\n",
      "Epoch 900, Test Loss: 0.5489224980796026, Accuracy: 0.9434977578475336, Precision: 0.9711538461538461, Recall: 0.6273291925465838, F1 Score: 0.7622641509433963\n",
      "Epoch 0, Validation Loss: 0.5075620939589278, Accuracy: 0.804932735426009, Precision: 0.058823529411764705, Recall: 0.03508771929824561, F1 Score: 0.04395604395604395\n",
      "Epoch 0, Test Loss: 0.5143337257586199, Accuracy: 0.8161434977578476, Precision: 0.14516129032258066, Recall: 0.055900621118012424, F1 Score: 0.08071748878923768\n",
      "Epoch 100, Validation Loss: 0.4657124268869379, Accuracy: 0.9013452914798207, Precision: 0.9333333333333333, Recall: 0.24561403508771928, F1 Score: 0.38888888888888884\n",
      "Epoch 100, Test Loss: 0.5157008041545983, Accuracy: 0.9049327354260089, Precision: 0.9230769230769231, Recall: 0.37267080745341613, F1 Score: 0.5309734513274336\n",
      "Epoch 200, Validation Loss: 0.4813614572512681, Accuracy: 0.9260089686098655, Precision: 0.98, Recall: 0.4298245614035088, F1 Score: 0.5975609756097562\n",
      "Epoch 200, Test Loss: 0.5430336583456292, Accuracy: 0.9345291479820628, Precision: 0.9489795918367347, Recall: 0.577639751552795, F1 Score: 0.7181467181467182\n",
      "Epoch 300, Validation Loss: 0.4880631966839682, Accuracy: 0.9316143497757847, Precision: 0.9818181818181818, Recall: 0.47368421052631576, F1 Score: 0.6390532544378698\n",
      "Epoch 300, Test Loss: 0.5529929359930064, Accuracy: 0.9417040358744395, Precision: 0.9615384615384616, Recall: 0.6211180124223602, F1 Score: 0.7547169811320754\n",
      "Epoch 400, Validation Loss: 0.4904476291418374, Accuracy: 0.9360986547085202, Precision: 0.9830508474576272, Recall: 0.5087719298245614, F1 Score: 0.6705202312138728\n",
      "Epoch 400, Test Loss: 0.5558387566044048, Accuracy: 0.947085201793722, Precision: 0.9636363636363636, Recall: 0.6583850931677019, F1 Score: 0.7822878228782288\n",
      "Epoch 500, Validation Loss: 0.491198754267093, Accuracy: 0.9383408071748879, Precision: 0.9836065573770492, Recall: 0.5263157894736842, F1 Score: 0.6857142857142856\n",
      "Epoch 500, Test Loss: 0.5563372255026092, Accuracy: 0.9506726457399103, Precision: 0.9649122807017544, Recall: 0.6832298136645962, F1 Score: 0.7999999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600, Validation Loss: 0.4914201408740213, Accuracy: 0.9405829596412556, Precision: 0.9841269841269841, Recall: 0.543859649122807, F1 Score: 0.7005649717514124\n",
      "Epoch 600, Test Loss: 0.556135085643123, Accuracy: 0.9533632286995516, Precision: 0.9739130434782609, Recall: 0.6956521739130435, F1 Score: 0.8115942028985508\n",
      "Epoch 700, Validation Loss: 0.491582544338867, Accuracy: 0.9405829596412556, Precision: 0.9841269841269841, Recall: 0.543859649122807, F1 Score: 0.7005649717514124\n",
      "Epoch 700, Test Loss: 0.5558880692351652, Accuracy: 0.9515695067264573, Precision: 0.9734513274336283, Recall: 0.6832298136645962, F1 Score: 0.8029197080291971\n",
      "Epoch 800, Validation Loss: 0.4916401485332831, Accuracy: 0.9405829596412556, Precision: 0.9841269841269841, Recall: 0.543859649122807, F1 Score: 0.7005649717514124\n",
      "Epoch 800, Test Loss: 0.5556311695408721, Accuracy: 0.9497757847533632, Precision: 0.972972972972973, Recall: 0.6708074534161491, F1 Score: 0.7941176470588235\n",
      "Epoch 900, Validation Loss: 0.49173078082925736, Accuracy: 0.9405829596412556, Precision: 0.9841269841269841, Recall: 0.543859649122807, F1 Score: 0.7005649717514124\n",
      "Epoch 900, Test Loss: 0.5554998329563418, Accuracy: 0.9488789237668162, Precision: 0.9727272727272728, Recall: 0.6645962732919255, F1 Score: 0.7896678966789669\n",
      "Epoch 0, Validation Loss: 0.5053643051368399, Accuracy: 0.8082959641255605, Precision: 0.06153846153846154, Recall: 0.03508771929824561, F1 Score: 0.04469273743016759\n",
      "Epoch 0, Test Loss: 0.5127439540090206, Accuracy: 0.820627802690583, Precision: 0.15789473684210525, Recall: 0.055900621118012424, F1 Score: 0.08256880733944955\n",
      "Epoch 100, Validation Loss: 0.46262597150297524, Accuracy: 0.8957399103139013, Precision: 0.92, Recall: 0.20175438596491227, F1 Score: 0.3309352517985611\n",
      "Epoch 100, Test Loss: 0.5142762705760842, Accuracy: 0.9022421524663677, Precision: 0.9482758620689655, Recall: 0.3416149068322981, F1 Score: 0.5022831050228309\n",
      "Epoch 200, Validation Loss: 0.4769346623395603, Accuracy: 0.9192825112107623, Precision: 0.9565217391304348, Recall: 0.38596491228070173, F1 Score: 0.5499999999999999\n",
      "Epoch 200, Test Loss: 0.5414053866679578, Accuracy: 0.9309417040358744, Precision: 0.9565217391304348, Recall: 0.546583850931677, F1 Score: 0.6956521739130436\n",
      "Epoch 300, Validation Loss: 0.4838721726323371, Accuracy: 0.9304932735426009, Precision: 0.9814814814814815, Recall: 0.4649122807017544, F1 Score: 0.6309523809523809\n",
      "Epoch 300, Test Loss: 0.5519140158084641, Accuracy: 0.9390134529147982, Precision: 0.9696969696969697, Recall: 0.5962732919254659, F1 Score: 0.7384615384615385\n",
      "Epoch 400, Validation Loss: 0.4865851327405427, Accuracy: 0.9327354260089686, Precision: 0.9821428571428571, Recall: 0.4824561403508772, F1 Score: 0.6470588235294117\n",
      "Epoch 400, Test Loss: 0.5552068235958386, Accuracy: 0.9461883408071748, Precision: 0.9719626168224299, Recall: 0.6459627329192547, F1 Score: 0.7761194029850746\n",
      "Epoch 500, Validation Loss: 0.48765675537621295, Accuracy: 0.9338565022421524, Precision: 0.9824561403508771, Recall: 0.49122807017543857, F1 Score: 0.6549707602339181\n",
      "Epoch 500, Test Loss: 0.5560509977948584, Accuracy: 0.947085201793722, Precision: 0.9722222222222222, Recall: 0.6521739130434783, F1 Score: 0.7806691449814127\n",
      "Epoch 600, Validation Loss: 0.48801767496278614, Accuracy: 0.9349775784753364, Precision: 0.9827586206896551, Recall: 0.5, F1 Score: 0.6627906976744186\n",
      "Epoch 600, Test Loss: 0.5559630046085426, Accuracy: 0.9479820627802691, Precision: 0.9724770642201835, Recall: 0.6583850931677019, F1 Score: 0.7851851851851852\n",
      "Epoch 700, Validation Loss: 0.48832822020748307, Accuracy: 0.9349775784753364, Precision: 0.9827586206896551, Recall: 0.5, F1 Score: 0.6627906976744186\n",
      "Epoch 700, Test Loss: 0.5558592285420342, Accuracy: 0.947085201793722, Precision: 0.9722222222222222, Recall: 0.6521739130434783, F1 Score: 0.7806691449814127\n",
      "Epoch 800, Validation Loss: 0.4884251830509715, Accuracy: 0.9360986547085202, Precision: 0.9830508474576272, Recall: 0.5087719298245614, F1 Score: 0.6705202312138728\n",
      "Epoch 800, Test Loss: 0.5556220223896029, Accuracy: 0.947085201793722, Precision: 0.9722222222222222, Recall: 0.6521739130434783, F1 Score: 0.7806691449814127\n",
      "Epoch 900, Validation Loss: 0.4886060096816794, Accuracy: 0.9360986547085202, Precision: 0.9830508474576272, Recall: 0.5087719298245614, F1 Score: 0.6705202312138728\n",
      "Epoch 900, Test Loss: 0.5555583194950796, Accuracy: 0.947085201793722, Precision: 0.9722222222222222, Recall: 0.6521739130434783, F1 Score: 0.7806691449814127\n",
      "Epoch 0, Validation Loss: 0.5091283360404933, Accuracy: 0.8038116591928252, Precision: 0.07042253521126761, Recall: 0.043859649122807015, F1 Score: 0.05405405405405406\n",
      "Epoch 0, Test Loss: 0.5154771485938201, Accuracy: 0.8143497757847533, Precision: 0.140625, Recall: 0.055900621118012424, F1 Score: 0.08000000000000002\n",
      "Epoch 100, Validation Loss: 0.4641018435282343, Accuracy: 0.897982062780269, Precision: 0.9259259259259259, Recall: 0.21929824561403508, F1 Score: 0.3546099290780142\n",
      "Epoch 100, Test Loss: 0.5147259401202687, Accuracy: 0.9004484304932735, Precision: 0.9310344827586207, Recall: 0.33540372670807456, F1 Score: 0.4931506849315069\n",
      "Epoch 200, Validation Loss: 0.47800873124060517, Accuracy: 0.922645739910314, Precision: 0.9787234042553191, Recall: 0.40350877192982454, F1 Score: 0.5714285714285714\n",
      "Epoch 200, Test Loss: 0.5402887267418545, Accuracy: 0.9282511210762332, Precision: 0.945054945054945, Recall: 0.5341614906832298, F1 Score: 0.6825396825396826\n",
      "Epoch 300, Validation Loss: 0.484288196820114, Accuracy: 0.9271300448430493, Precision: 0.9803921568627451, Recall: 0.43859649122807015, F1 Score: 0.6060606060606061\n",
      "Epoch 300, Test Loss: 0.5498949955257425, Accuracy: 0.9327354260089686, Precision: 0.9574468085106383, Recall: 0.5590062111801242, F1 Score: 0.7058823529411764\n",
      "Epoch 400, Validation Loss: 0.4864661100705889, Accuracy: 0.9327354260089686, Precision: 0.9821428571428571, Recall: 0.4824561403508772, F1 Score: 0.6470588235294117\n",
      "Epoch 400, Test Loss: 0.5526683075870873, Accuracy: 0.9408071748878923, Precision: 0.9611650485436893, Recall: 0.6149068322981367, F1 Score: 0.7500000000000001\n",
      "Epoch 500, Validation Loss: 0.48735716882048935, Accuracy: 0.9372197309417041, Precision: 0.9833333333333333, Recall: 0.5175438596491229, F1 Score: 0.6781609195402298\n",
      "Epoch 500, Test Loss: 0.5534338036046518, Accuracy: 0.9426008968609866, Precision: 0.970873786407767, Recall: 0.6211180124223602, F1 Score: 0.7575757575757577\n",
      "Epoch 600, Validation Loss: 0.4876531792053992, Accuracy: 0.9383408071748879, Precision: 0.9836065573770492, Recall: 0.5263157894736842, F1 Score: 0.6857142857142856\n",
      "Epoch 600, Test Loss: 0.5534289654872937, Accuracy: 0.9426008968609866, Precision: 0.970873786407767, Recall: 0.6211180124223602, F1 Score: 0.7575757575757577\n",
      "Epoch 700, Validation Loss: 0.4879376851951231, Accuracy: 0.9383408071748879, Precision: 0.9836065573770492, Recall: 0.5263157894736842, F1 Score: 0.6857142857142856\n",
      "Epoch 700, Test Loss: 0.553374889332862, Accuracy: 0.9426008968609866, Precision: 0.970873786407767, Recall: 0.6211180124223602, F1 Score: 0.7575757575757577\n",
      "Epoch 800, Validation Loss: 0.48802116949920726, Accuracy: 0.9383408071748879, Precision: 0.9836065573770492, Recall: 0.5263157894736842, F1 Score: 0.6857142857142856\n",
      "Epoch 800, Test Loss: 0.553217241461173, Accuracy: 0.9443946188340807, Precision: 0.9714285714285714, Recall: 0.6335403726708074, F1 Score: 0.7669172932330828\n",
      "Epoch 900, Validation Loss: 0.48824546085492687, Accuracy: 0.9383408071748879, Precision: 0.9836065573770492, Recall: 0.5263157894736842, F1 Score: 0.6857142857142856\n",
      "Epoch 900, Test Loss: 0.5532813910378105, Accuracy: 0.9434977578475336, Precision: 0.9711538461538461, Recall: 0.6273291925465838, F1 Score: 0.7622641509433963\n",
      "Epoch 0, Validation Loss: 0.8375285101687374, Accuracy: 0.8195067264573991, Precision: 0.05660377358490566, Recall: 0.02631578947368421, F1 Score: 0.03592814371257485\n",
      "Epoch 0, Test Loss: 0.778599221719014, Accuracy: 0.8278026905829596, Precision: 0.1702127659574468, Recall: 0.049689440993788817, F1 Score: 0.07692307692307691\n",
      "Epoch 100, Validation Loss: 0.39155140847831693, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 100, Test Loss: 0.4254223120518609, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, Validation Loss: 0.39111041790355633, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 200, Test Loss: 0.4247450366545694, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 300, Validation Loss: 0.39095934181335307, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 300, Test Loss: 0.42501906213118035, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 400, Validation Loss: 0.3909869036193078, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 400, Test Loss: 0.42498819593226217, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 500, Validation Loss: 0.39102941035992084, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 500, Test Loss: 0.42484602823163103, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 600, Validation Loss: 0.39099117672704825, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 600, Test Loss: 0.4251625228052889, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 700, Validation Loss: 0.39098843912574277, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 700, Test Loss: 0.4250748078430049, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 800, Validation Loss: 0.39104703666315976, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 800, Test Loss: 0.42484280411701497, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 900, Validation Loss: 0.3909785011171693, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 900, Test Loss: 0.4250473842074154, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 0, Validation Loss: 0.8372046653624107, Accuracy: 0.8195067264573991, Precision: 0.05660377358490566, Recall: 0.02631578947368421, F1 Score: 0.03592814371257485\n",
      "Epoch 0, Test Loss: 0.7783692060163696, Accuracy: 0.8278026905829596, Precision: 0.1702127659574468, Recall: 0.049689440993788817, F1 Score: 0.07692307692307691\n",
      "Epoch 100, Validation Loss: 0.3913795576459988, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 100, Test Loss: 0.4252320091978375, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 200, Validation Loss: 0.39082324233169996, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 200, Test Loss: 0.42458547175224565, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 300, Validation Loss: 0.390813420385636, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 300, Test Loss: 0.42499984667906326, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 400, Validation Loss: 0.39082793405934513, Accuracy: 0.8721973094170403, Precision: 0, Recall: 0.0, F1 Score: 0\n",
      "Epoch 400, Test Loss: 0.42467218066334, Accuracy: 0.8556053811659193, Precision: 0, Recall: 0.0, F1 Score: 0\n"
     ]
    }
   ],
   "source": [
    "# Define a range of lambda values to test\n",
    "lambda_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Calculate the size of each fold\n",
    "fold_size = X_train.shape[0] // n_folds\n",
    "\n",
    "# Split the training data indices into k-folds\n",
    "folds = [range(i * fold_size, (i + 1) * fold_size) for i in range(n_folds)]\n",
    "\n",
    "# Initialize dictionary to store the average performance for each lambda\n",
    "performance_dict = {lmbd: [] for lmbd in lambda_values}\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for lambda_reg in lambda_values:\n",
    "    for fold in range(n_folds):\n",
    "        # Determine the indices for this fold\n",
    "        val_indices = folds[fold]\n",
    "        train_indices = [i for i in range(X_train.shape[0]) if i not in val_indices]\n",
    "\n",
    "        # Extract training and validation sets from the indices\n",
    "        X_train_fold = X_train[train_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        X_val_fold = X_train[val_indices]\n",
    "        y_val_fold = y_train.iloc[val_indices]\n",
    "\n",
    "        # Train your model here with the given lambda_reg\n",
    "        model = mini_batch(X_train_fold, y_train_fold, lambda_reg)\n",
    "\n",
    "    \n",
    "        # Store the performance metric\n",
    "        performance_dict[lambda_reg].append(model)\n",
    "\n",
    "# Calculate the average performance for each lambda\n",
    "average_performance = {lmbd: sum(perfs) / len(perfs) for lmbd, perfs in performance_dict.items()}\n",
    "\n",
    "# Select the best lambda\n",
    "best_lambda = min(average_performance, key=average_performance.get)\n",
    "\n",
    "# Retrain model on the entire training set using the best lambda\n",
    "final_model = mini_batch(X_train, y_train, best_lambda)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "final_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ebb04",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "#### Introduction\n",
    "This section of the report evaluates the performance of a text classification model, specifically designed to distinguish between different categories of text data (such as 'spam' vs. 'ham' in emails). The model's efficacy was assessed based on various metrics, including accuracy, precision, recall, F1 score, and loss. A key aspect of this evaluation involved tuning the regularization parameter, \\(\\lambda\\), to optimize model performance.\n",
    "\n",
    "#### Methodology\n",
    "The model can be trained using a stochastic gradient descent algorithm or a mini batch gradient descent algorithm. To combat potential overfitting and improve generalization, a regularization term parameterized by \\(\\lambda\\) was introduced. The optimal value for \\(\\lambda\\) was determined through a manual implementation of k-fold cross-validation, given the constraint of not using external libraries like scikit-learn for this purpose.\n",
    "\n",
    "#### Results\n",
    "Initial results showed a significant decrease in both validation and test loss after the first 20 epochs, stabilizing thereafter. However, while accuracy remained high, the precision, recall, and F1 scores were low, particularly in identifying the minority class in our dataset. This suggests a model bias towards the majority class.\n",
    "\n",
    "Post tuning, the model with the optimal \\(\\lambda\\) showed a slight improvement in balancing the recall and precision, indicating a better handling of the class imbalance. However, the F1 scores remained lower than desired, suggesting room for further model refinement.\n",
    "\n",
    "#### Challenges\n",
    "Key challenges in this process included:\n",
    "- Handling class imbalance, which skewed initial model performance metrics.\n",
    "- Implementing manual cross-validation, which is more prone to errors and requires careful handling of data splits.\n",
    "- The limitation of not using advanced libraries like scikit-learn, which meant more complex tasks had to be implemented manually.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "#### Summary\n",
    "The exercise demonstrated the importance and impact of regularization in text classification models. While the model achieved high accuracy, it initially struggled with precision and recall, likely due to class imbalance. The manual tuning of the regularization parameter \\(\\lambda\\) through cross-validation helped improve the model's ability to generalize, though not to the desired extent.\n",
    "\n",
    "#### Learnings\n",
    "This task highlighted the critical nature of data preprocessing, the need for robust evaluation metrics in the presence of class imbalances, and the complexities involved in manual model tuning.\n",
    "\n",
    "#### Resources\n",
    "handbook:\n",
    "Hands-On Machine Learning with Scikit-Learn, keras and TensorFlow by Aurelien Geron \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
