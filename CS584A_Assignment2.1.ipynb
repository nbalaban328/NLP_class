{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a1ee68",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
    "\n",
    "#### Name: Nyrah Balabanian\n",
    "#### Stevens ID: 20005955"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21f22a",
   "metadata": {},
   "source": [
    "## Part A: Multi-Layer Perceptron (MLP) (50 Points)\n",
    "\n",
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the data loading, preprocessing, tokenization, and TF-IDF feature extraction.\n",
    "3. Implement MLP model, evaluation metrics, and Mini-batch GD with AdaGrad.\n",
    "4. Implement the MLP with Tensorflow and compare to your implementation.\n",
    "5. Analysis the results in the Conlusion part.\n",
    "\n",
    "**Before you start**\n",
    "- Please read the code very carefully.\n",
    "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons, pandas) using the following command.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
    "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
    "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a85831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyterlab in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (3.5.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: numpy in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (2.13.0)\n",
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.22.0-cp310-cp310-win_amd64.whl (719 kB)\n",
      "     ---------------------------------------- 0.0/719.8 kB ? eta -:--:--\n",
      "     -------------------------------- ---- 624.6/719.8 kB 19.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- 719.8/719.8 kB 11.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: ipython in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (8.11.0)\n",
      "Requirement already satisfied: nbclassic in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (0.5.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (5.2.0)\n",
      "Requirement already satisfied: notebook<7 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (6.5.2)\n",
      "Requirement already satisfied: tomli in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: jupyterlab-server~=2.10 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.19.0)\n",
      "Requirement already satisfied: tornado>=6.1.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (6.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: jinja2>=2.1 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: jupyter-server<3,>=1.16.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r requirements.txt (line 2)) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 3)) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 3)) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from nltk->-r requirements.txt (line 3)) (8.1.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow->-r requirements.txt (line 6)) (2.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (1.56.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (23.5.26)\n",
      "Requirement already satisfied: setuptools in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (65.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (4.21.12)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (0.31.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (2.13.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (16.0.6)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (2.13.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (2.13.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (3.7.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (1.14.1)\n",
      "Collecting typeguard<3.0.0,>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jinja2>=2.1->jupyterlab->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (7.2.9)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (8.0.3)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (25.0.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.17.1)\n",
      "Requirement already satisfied: jupyter-server-terminals in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.4.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (5.7.3)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (3.6.2)\n",
      "Requirement already satisfied: send2trash in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: jupyter-events>=0.4.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (5.9.0)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.58.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (21.3.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: pywinpty in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.0.10)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 1)) (2.5.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 1)) (305.1)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.17.3 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (4.17.3)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.11.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from notebook<7->jupyterlab->-r requirements.txt (line 1)) (1.5.6)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from notebook<7->jupyterlab->-r requirements.txt (line 1)) (6.21.2)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from notebook<7->jupyterlab->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from nbclassic->jupyterlab->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk->-r requirements.txt (line 3)) (0.4.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (2.12.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (3.0.38)\n",
      "Requirement already satisfied: decorator in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from ipython->jupyterlab->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (0.38.4)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from babel>=2.10->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2022.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython->jupyterlab->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (22.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (0.19.3)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.4.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.1.4)\n",
      "Requirement already satisfied: bleach in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (6.0.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.7.2)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.0.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (4.11.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.16.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.28->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from requests>=2.28->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from requests>=2.28->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (0.7.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (21.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->notebook<7->jupyterlab->-r requirements.txt (line 1)) (1.6.6)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->notebook<7->jupyterlab->-r requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->notebook<7->jupyterlab->-r requirements.txt (line 1)) (5.9.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from stack-data->ipython->jupyterlab->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (5.3.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (2.3)\n",
      "Requirement already satisfied: uri-template in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: fqdn in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.4)\n",
      "Requirement already satisfied: webencodings in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab->-r requirements.txt (line 1)) (2.21)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\scoop\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->-r requirements.txt (line 6)) (3.2.2)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\scoop\\appdata\\roaming\\python\\python310\\site-packages (from isoduration->jsonschema>=4.17.3->jupyterlab-server~=2.10->jupyterlab->-r requirements.txt (line 1)) (1.2.3)\n",
      "Installing collected packages: typeguard, tensorflow_addons\n",
      "Successfully installed tensorflow_addons-0.22.0 typeguard-2.13.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# you may not run this cell after the first installation\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2361dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2c681",
   "metadata": {},
   "source": [
    "## 1. Data Processing (5 points)\n",
    "\n",
    "* Download the dataset from Canvas\n",
    "* Load data to text and labels\n",
    "* Preprocessing\n",
    "* Tokenization\n",
    "* Split data\n",
    "* Feature extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb9b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2ea26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# Open the file in read mode\n",
    "with open('books.txt', 'r',  encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "    for line in file:\n",
    "        # Parse the line (by tab)\n",
    "        parsed_line = line.strip().split('\t')\n",
    "        \n",
    "        # Add the parsed data to the list\n",
    "        data.append(parsed_line)\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df_raw = pd.DataFrame(data, columns=['label', 'text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030e7b7",
   "metadata": {},
   "source": [
    "#### Download NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a55e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to a2-data\\nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk_path = os.path.join('a2-data', 'nltk')\n",
    "nltk.download('stopwords', download_dir=nltk_path)\n",
    "nltk.data.path.append(nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3e61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    sys.stdout.write(str_ + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0f24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2358002",
   "metadata": {},
   "source": [
    "### 1.1 Load data\n",
    "\n",
    "- Load sentences and labels\n",
    "- Transform string labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c00f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_label(data_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\" Load sentences and labels from the specified path\n",
    "    Args:\n",
    "        data_path: data_path: path to the data file, e.g., 'a1-data/SMSSpamCollection'\n",
    "        sentences: the raw text list of all sentences\n",
    "    Returns:\n",
    "        labels: the label list of all sentences\n",
    "    \"\"\"\n",
    "    sentences, labels = [], []\n",
    "    # Start your code here (load text and label from files)\n",
    "    with open('books.txt', 'r', encoding='utf-8') as file: \n",
    "        for line in file:\n",
    "            parsed_line = line.strip().split('\\t')\n",
    "            if len(parsed_line) == 2:\n",
    "                label, sentence = parsed_line\n",
    "                labels.append(label)\n",
    "                sentences.append(sentence)\n",
    "            else:\n",
    "                # Handle unexpected line format\n",
    "                print(f\"Unexpected format in line: {line}\")\n",
    "\n",
    "    # End\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a85fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'Arthur Conan Doyle': 0, 'Fyodor Dostoyevsky': 1, 'Jane Austen': 2}\n",
      "Number of sentences and labels: 19536 19536\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('a2-data', 'books.txt')\n",
    "sentences, labels = load_sentence_label(data_path)\n",
    "\n",
    "label_map = {}\n",
    "for label in sorted(list(set(labels))):\n",
    "    label_map[label] = len(label_map)\n",
    "labels = np.array([label_map[label] for label in labels], dtype=int)\n",
    "sentences = np.array(sentences, dtype=object)\n",
    "\n",
    "print('Label map:', label_map)\n",
    "print('Number of sentences and labels:', len(sentences), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a5911",
   "metadata": {},
   "source": [
    "#### Split the data into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67728fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(sentences: np.ndarray,\n",
    "                     labels: np.ndarray,\n",
    "                     test_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Split the sentences and labels into training and test data by shuffling\n",
    "    Args:\n",
    "        sentences: A numpy array containing all sentences\n",
    "        labels: A number array containing label ids\n",
    "        test_ratio: A float number to calculate the number of test data\n",
    "\n",
    "    Returns:\n",
    "        train_sentences: A numpy array containing all training sentences\n",
    "        train_labels: A number array containing all training label ids\n",
    "        test_sentences: A numpy array containing all test sentences\n",
    "        test_labels: A number array containing all test label ids\n",
    "    \"\"\"\n",
    "    assert 0 < test_ratio < 1\n",
    "    assert len(sentences) == len(labels)\n",
    "\n",
    "    train_index, test_index = [], []\n",
    "    # Start your code here (split the index for training and test)\n",
    "    \n",
    "     # Shuffling indices\n",
    "    indices = np.arange(len(sentences))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Calculating split index\n",
    "    test_size = int(len(sentences) * test_ratio)\n",
    "    test_index = indices[:test_size]\n",
    "    train_index = indices[test_size:]\n",
    "\n",
    "    # End\n",
    "\n",
    "    train_sentences, train_labels = sentences[train_index], labels[train_index]\n",
    "    test_sentences, test_labels = sentences[test_index], labels[test_index]\n",
    "    return train_sentences, train_labels, test_sentences, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4678c731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 14067\n",
      "Validation data length: 1562\n",
      "Test data length: 3907\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.1\n",
    "(train_sentences, train_labels,\n",
    "    test_sentences, test_labels) = train_test_split(sentences, labels, test_ratio)\n",
    "(train_sentences, train_labels,\n",
    "    valid_sentences, valid_labels) = train_test_split(train_sentences, train_labels, valid_ratio)\n",
    "\n",
    "print('Training data length:', len(train_sentences))\n",
    "print('Validation data length:', len(valid_sentences))\n",
    "print('Test data length:', len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac59ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_label(labels: np.ndarray, label_map: dict[str, int]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: The labels of a dataset \n",
    "        label_map: The mapping from label to label id\n",
    "    Returns:\n",
    "        label_count: The mapping from label to its count\n",
    "    \"\"\"\n",
    "    label_count = {key: 0 for key in label_map.keys()}\n",
    "    # Start your code here (count the number of each label)\n",
    "    label_count = np.bincount(labels)\n",
    "\n",
    "    # End\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae2e1ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [1870 4234 7963]\n",
      "Validation: [193 464 905]\n",
      "Test: [ 475 1246 2186]\n"
     ]
    }
   ],
   "source": [
    "print('Training:', count_label(train_labels, label_map))\n",
    "print('Validation:', count_label(valid_labels, label_map))\n",
    "print('Test:', count_label(test_labels, label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909bd4c",
   "metadata": {},
   "source": [
    "#### Dataset statistics\n",
    "Fill this table with the statistics you just printed (double click this cell to edit)\n",
    "\n",
    "|                | Arthur Conan Doyle | Fyodor Dostoyevsky | Jane Austen | Total |\n",
    "|:--------------:|--------------------|--------------------|-------------|-------|\n",
    "|  **Training**  |         1870       |         4234       |    7963     |       |\n",
    "| **Validation** |         193        |       464          |     905     |       |\n",
    "|    **Test**    |         475        |         1246       |     2186    |       |\n",
    "|    **Total**   |        2538        |         5944       |     11054   |       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d0797",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess\n",
    "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ddcfbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "\n",
    "    def apply(self, sentence: str) -> str:\n",
    "        \"\"\" Apply the preprocessing rules to the sentence\n",
    "        Args:\n",
    "            sentence: raw sentence\n",
    "        Returns:\n",
    "            sentence: clean sentence\n",
    "        \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        if self.url:\n",
    "            sentence = Preprocessor.remove_url(sentence)\n",
    "        if self.punctuation:\n",
    "            sentence = Preprocessor.remove_punctuation(sentence)\n",
    "        if self.number:\n",
    "            sentence = Preprocessor.remove_number(sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(sentence: str) -> str:\n",
    "        \"\"\" Remove punctuations in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible punctuations\n",
    "        Returns:\n",
    "            sentence: sentence without punctuations\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        \n",
    "        # Remove punctuation\n",
    "        sentence = re.sub(r\"[^a-zA-Z' ]\", '', sentence)\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(sentence: str) -> str:\n",
    "        \"\"\" Remove urls in text with re\n",
    "        Args:\n",
    "            sentence: sentence with possible urls\n",
    "        Returns:\n",
    "            sentence: sentence without urls\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Remove URLs\n",
    "        sentence = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_number(sentence: str) -> str:\n",
    "        \"\"\" Remove numbers in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible numbers\n",
    "        Returns:\n",
    "            sentence: sentence without numbers\n",
    "        \"\"\"\n",
    "        # Start your code here \n",
    "        sentence = re.sub(r\"[^a-zA-Z' ]\", '', sentence)\n",
    "\n",
    "\n",
    "        # End\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48330cff",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39d6e669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"interest rates are trimmed to by the south african central bank but the lack of warning hits the rand and surprises markets\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{clean_sentence}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54d3a0",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd6b643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they', \"you'd\", 'wouldn', 'here', 'such', 'after', 'we', 'you', 'by', 'have']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "print(list(stopwords_set)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5df4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\" Tokenize a sentence into tokens (words)\n",
    "    Args:\n",
    "        sentence: clean sentence\n",
    "    Returns:\n",
    "        tokens\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    # Start your code here\n",
    "    #     Step 1. Split sentence into words\n",
    "    #     Step 2. Extract word stem using the defined stemmer (PorterStemmer) by calling stemmer.stem(word)\n",
    "    #     Step 3. Remove stop words using the defined stopwords_set\n",
    "\n",
    "    tokens = word_tokenize(sentence)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in stemmed_tokens if word not in stop_words]\n",
    "\n",
    "\n",
    "    # End\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d3c30",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc088f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"['interest', 'rate', 'trim', 'south', 'african', 'central', 'bank', 'lack', 'warn', 'hit', 'rand', 'surpris', 'market']\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "tokens = tokenize(clean_sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{tokens}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753be3c",
   "metadata": {},
   "source": [
    "### 1.5 Feature Extraction\n",
    "\n",
    "TF-IDF:\n",
    "$$\\text{TF-IDF}(t, d) = \\frac{f_{t, d}}{\\sum_{t'}{f_{t', d}}} \\times \\log{\\frac{N}{n_t}}$$\n",
    "\n",
    "- $t$: A term\n",
    "- $d$: A document. Here, we regard a sentence as a document\n",
    "- $f_{t, d}$: Number of term $t$ in $d$\n",
    "- $N$: Number of document\n",
    "- $n_t$: Number of document containing $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4e1635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TfIdfEncoder:\n",
    "    def __init__(self):\n",
    "        self.vocab = defaultdict(int)\n",
    "        self.token2index = {}\n",
    "        self.df = defaultdict(int)\n",
    "        self.num_doc = 0\n",
    "        self.processor = Preprocessor()\n",
    "\n",
    "    def fit(self, sentences: Union[List[str], np.ndarray]) -> int:\n",
    "        \"\"\" Using the given texts to store key information in TF-IDF calculation\n",
    "            In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary and store the frequency of tokens (self.vocab).\n",
    "                2. Construct the document frequency map to tokens (self.df).\n",
    "                3. Construct the token to index map based on the frequency.\n",
    "                   The token with a higher frequency has the smaller index\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            token_num\n",
    "        \"\"\"\n",
    "        self.num_doc = len(sentences)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == len(sentences) - 1:\n",
    "                print_line('Fitting TF-IDF encoder:', (i + 1), '/', len(sentences))\n",
    "            # Start your code here (step 1 & 2)\n",
    "            tokens = set(tokenize(sentence))\n",
    "            for token in tokens:\n",
    "                # Increase document frequency\n",
    "                self.df[token] += 1\n",
    "                # Increase term frequency\n",
    "                self.vocab[token] += 1\n",
    "            \n",
    "\n",
    "            # End\n",
    "        print_line('\\n')\n",
    "        # Start your code here (Step 3)\n",
    "        sorted_vocab = sorted(self.vocab.items(), key=lambda item: item[1], reverse=True)\n",
    "        self.token2index = {token: idx for idx, (token, _) in enumerate(sorted_vocab)}\n",
    "\n",
    "\n",
    "        # End\n",
    "        token_num = len(self.token2index) \n",
    "        print('The number of distinct tokens:', token_num)\n",
    "        return token_num\n",
    "\n",
    "    def encode(self, sentences: Union[List[str], np.ndarray]) -> np.ndarray:\n",
    "        \"\"\" Encode the sentences into TF-IDF feature vector\n",
    "            Note: if a token in a sentence does not exist in the fit encoder, we just ignore it.\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            features: A (n x token_num) matrix, where n is the number of sentences\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        features = np.zeros((n, len(self.token2index)))\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Encoding with TF-IDF encoder:', (i + 1), '/', n)\n",
    "            # Start your code (calculate TF-IDF)\n",
    "            # Tokenize the sentence\n",
    "            tokens = tokenize(sentence)\n",
    "            # Calculate term frequency in sentence\n",
    "            tf = defaultdict(int)\n",
    "            for token in tokens:\n",
    "                if token in self.token2index:\n",
    "                    tf[token] += 1\n",
    "\n",
    "            # Calculate TF-IDF for each token\n",
    "            for token, freq in tf.items():\n",
    "                if token in self.token2index:\n",
    "                    tf_idf = (freq / len(tokens)) * np.log(self.num_doc / (1 + self.df[token]))\n",
    "                    features[i, self.token2index[token]] = tf_idf\n",
    "\n",
    "\n",
    "            # End\n",
    "        print_line('\\n')\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc08dc",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a8ba48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 100 / 100\n",
      "The number of distinct tokens: 1475\n",
      "Encoding with TF-IDF encoder: 10 / 10\n",
      "[[0.01111595 0.02826416 0.05686317 ... 0.         0.         0.        ]\n",
      " [0.00672807 0.03421451 0.06883436 ... 0.         0.         0.        ]\n",
      " [0.01217461 0.01547799 0.         ... 0.         0.         0.        ]\n",
      " [0.01597917 0.02031487 0.         ... 0.         0.         0.        ]\n",
      " [0.00983334 0.02327857 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "encoder = TfIdfEncoder()\n",
    "encoder.fit(train_sentences[:100])\n",
    "features = encoder.encode(train_sentences[:10])\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369c8be",
   "metadata": {},
   "source": [
    "#### Encode training, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd20ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 14067 / 14067\n",
      "The number of distinct tokens: 14693\n",
      "Encoding with TF-IDF encoder: 14067 / 14067\n",
      "Encoding with TF-IDF encoder: 1562 / 1562\n",
      "Encoding with TF-IDF encoder: 3907 / 3907\n",
      "The size of training set: (14067, 14693) (14067, 3)\n",
      "The size of validation set: (1562, 14693) (1562, 3)\n",
      "The size of test set: (3907, 14693) (3907, 3)\n"
     ]
    }
   ],
   "source": [
    "num_class = 3\n",
    "\n",
    "encoder = TfIdfEncoder()\n",
    "vocab_size = encoder.fit(train_sentences)\n",
    "\n",
    "x_train = encoder.encode(train_sentences)\n",
    "x_valid = encoder.encode(valid_sentences)\n",
    "x_test = encoder.encode(test_sentences)\n",
    "\n",
    "y_train = np.zeros((len(train_labels), num_class))\n",
    "y_valid = np.zeros((len(valid_labels), num_class))\n",
    "y_test = np.zeros((len(test_labels), num_class))\n",
    "y_train[np.arange(len(train_labels)), train_labels] = 1\n",
    "y_valid[np.arange(len(valid_labels)), valid_labels] = 1\n",
    "y_test[np.arange(len(test_labels)), test_labels] = 1\n",
    "\n",
    "print('The size of training set:', x_train.shape, y_train.shape)\n",
    "print('The size of validation set:', x_valid.shape, y_valid.shape)\n",
    "print('The size of test set:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37588ff6",
   "metadata": {},
   "source": [
    "## 2. MLP (20 Points)\n",
    "In this section, you are required to implement a two-layer MLP model (input -> hidden layer -> output layer) with $L_2$ regularization from scratch. \n",
    "\n",
    "The objective function of LR for multi-class classification:\n",
    "\n",
    "$$J = L(\\mathbf{x}, \\mathbf{y} \\mid \\mathbf{w}, \\mathbf{b}) = -\\frac{1}{n}\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik}log\\frac{e^{f_k}}{\\sum_{c=1}^{K}e^{f_c}} + \\lambda \\sum_{j=1}^{d}w_{kj}^2$$\n",
    "\n",
    "- $z_1 = w_1x$\n",
    "- $h_1 = activation(z_1)$\n",
    "- $z_2 = w_2 h_1$\n",
    "- $\\hat{y} = softmax(z_2)$\n",
    "\n",
    "- $n$: Number of samples\n",
    "- $d$: Dimension of $\\mathbf{w}$\n",
    "- Here, you can use `sigmoid` as the activation function for the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fb8a5",
   "metadata": {},
   "source": [
    "### 2.1 MLP Model (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efe001dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\" The softmax activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "        axis: The dimension of x that needs to run softmax, default -1, i.e., the last dimension\n",
    "    Returns:\n",
    "        output: Softmax value of the specified dimension in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    x = e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "    # End\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" The sigmoid activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "    Returns:\n",
    "        output: Sigmoid value of each entry in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # End\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fdd7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        self.w1 = np.random.randn(feature_dim, hidden_dim) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.w2 = np.random.randn(hidden_dim, num_class) * 0.01\n",
    "        self.b2 = np.zeros((1, num_class))\n",
    "\n",
    "        # End\n",
    "        self.lambda_ = lambda_\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, x: np.ndarray, return_hiddens: bool = False) -> np.ndarray:\n",
    "        \"\"\" Forward process of logistic regression\n",
    "            Calculate y_hat using x\n",
    "        Args:\n",
    "            x: Input data\n",
    "            return_hiddens: If true the function will return h1 for gradient calculation\n",
    "        Returns:\n",
    "            y_hat: Output\n",
    "            h1: Hidden output, used for gradient calculation. Returned if return_hiddens is set to True\n",
    "        \"\"\"\n",
    "        y_hat = 0\n",
    "        h1 = 0, 0\n",
    "        w1, b1, w2, b2 = self.w1, self.b1, self.w2, self.b2\n",
    "        # Start your code here (calculate y_hat of MLP using x)\n",
    "        # Calculate the hidden layer\n",
    "        z1 = np.dot(x,self.w1) + self.b1\n",
    "        h1 = sigmoid(z1)\n",
    "\n",
    "        # Calculate the output layer\n",
    "        z2 = np.dot(h1, self.w2) + self.b2\n",
    "        y_hat = softmax(z2)\n",
    "\n",
    "        # End\n",
    "        if return_hiddens:\n",
    "            return y_hat, h1\n",
    "        else:\n",
    "            return y_hat\n",
    "\n",
    "    def backward(self,\n",
    "                 x: np.ndarray,\n",
    "                 y_hat: np.ndarray,\n",
    "                 y: np.ndarray,\n",
    "                 h1: np.array) -> Tuple[np.ndarray, Union[float, np.ndarray], np.ndarray, Union[float, np.ndarray]]:\n",
    "        \"\"\" Backward process of logistic regression\n",
    "            Calculate the gradient of w and b\n",
    "        Args:\n",
    "            x: Input data\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "            h1: Hidden output of the hidden layer\n",
    "        Returns:\n",
    "            dw1: Gradient of w1\n",
    "            db1: Gradient of b1\n",
    "            dw2: Gradient of w2\n",
    "            db2: Gradient of b2\n",
    "        \"\"\"\n",
    "        w1, w2 = self.w1, self.w2\n",
    "        #dw1, db1, dw2, db2 = 0.0, 0.0, 0.0, 0.0\n",
    "        n = len(x)\n",
    "        # Start your code here (calculate the gradient of w and b)\n",
    "        \n",
    "        # Gradient of loss w.r.t. softmax input\n",
    "        d_loss = y_hat - y\n",
    "\n",
    "        # Gradient of weights and biases at output layer\n",
    "        dw2 = np.dot(np.transpose(h1), d_loss) / n + self.lambda_ * self.w2\n",
    "        db2 = np.sum(d_loss, axis=0) / n\n",
    "\n",
    "        # Backpropagate to hidden layer\n",
    "        d_hidden = np.dot(d_loss, self.w2.T) * h1 * (1 - h1)\n",
    "\n",
    "        # Gradient of weights and biases at hidden layer\n",
    "        dw1 = np.dot(np.transpose(x), d_hidden) / n + self.lambda_ * self.w1\n",
    "        db1 = np.sum(d_hidden, axis=0) / n\n",
    "        \n",
    "        \n",
    "\n",
    "        # End\n",
    "        return dw1, db1, dw2, db2\n",
    "\n",
    "    def categorical_cross_entropy_loss(self,\n",
    "                                       y_hat: np.ndarray,\n",
    "                                       y: np.ndarray) -> Union[float, np.ndarray]:\n",
    "        \"\"\" Calculate the binary cross-entropy loss\n",
    "        Args:\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "        Returns:\n",
    "            loss: BCE loss\n",
    "        \"\"\"\n",
    "        y_hat = np.clip(y_hat, a_min=self.eps, a_max=1 - self.eps)\n",
    "        #loss = 0\n",
    "        # Start your code here (Calculate the binary cross-entropy)\n",
    "        # Calculate categorical cross-entropy loss\n",
    "        loss = -np.sum(y * np.log(y_hat)) / len(y)\n",
    "\n",
    "        # End\n",
    "        return loss\n",
    "\n",
    "    def gradient_descent(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float], lr: float):\n",
    "        self.w1 -= lr * dw1\n",
    "        self.b1 -= lr * db1\n",
    "        self.w2 -= lr * dw2\n",
    "        self.b2 -= lr * db2\n",
    "\n",
    "    def predict(self, y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict the label using the output y_hat\n",
    "        Args:\n",
    "            y_hat: Model output\n",
    "        Returns:\n",
    "            pred: Prediction\n",
    "        \"\"\"\n",
    "        pred = np.zeros_like(y_hat)\n",
    "        index = np.argmax(y_hat, axis=-1)\n",
    "        pred[np.arange(len(y_hat)), index] = 1\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2e514",
   "metadata": {},
   "source": [
    "### 2.2 Evaluation Metrics\n",
    "\n",
    "Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ea10c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def get_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Calculate the accuracy, precision, recall, and f1 score.\n",
    "        You are allowed to use precision_recall_fscore_support from scikit-learn. Please set average to 'micro'\n",
    "    Args:\n",
    "        y_pred: Prediction\n",
    "        y_true: Ground-truth\n",
    "    Returns:\n",
    "        accuracy: float number. The accuracy for the whole dataset\n",
    "        precision, recall, f1: np.ndarray (num_class, ). The precision, recall, f1 for each class\n",
    "    \"\"\"\n",
    "    y_true = y_true.reshape(-1)\n",
    "    print(\"y_pred: \", y_pred.shape)\n",
    "    print(\"y_true: \", y_true.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    assert y_pred.shape == y_true.shape\n",
    "    accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "    # Start your code here\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y_pred == y_true)\n",
    "\n",
    "    # Calculate precision, recall, and f1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "\n",
    "\n",
    "    # End\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ac467",
   "metadata": {},
   "source": [
    "### 2.3 AdaGrad (5 points)\n",
    "\n",
    "$$ \\mathbf{G}^{(t + 1)} \\leftarrow \\mathbf{G}^{(t)} + \\boldsymbol{g}^{(t + 1)} \\cdot \\boldsymbol{g}^{(t + 1)} $$\n",
    "$$ \\mathbf{w}^{(t + 1)} \\leftarrow \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}}\\boldsymbol{g}^{(t + 1)} = \\mathbf{w}^{(t)} - \\eta\\frac{\\boldsymbol{g}^{(t + 1)}}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d856e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, init_lr, model):\n",
    "        self.init_lr = init_lr\n",
    "        self.model = model\n",
    "        \n",
    "        self.accumulative_dw1 = 0\n",
    "        self.accumulative_db1 = 0\n",
    "        self.accumulative_dw2 = 0\n",
    "        self.accumulative_db2 = 0\n",
    "        self.eps = 1e-9\n",
    "        \n",
    "    def update(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float]):\n",
    "        \"\"\" 1. Use the gradient in the current step to update the accumulative gradient of each parameter.\n",
    "            2. Calculate the new gradient with the accumulative gradient\n",
    "            3. Use the init learning rate the new gradient to update the parameter with model.gradient_descent()\n",
    "        \n",
    "        Do not return anything\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1\n",
    "        self.accumulative_dw1 += dw1 ** 2\n",
    "        self.accumulative_db1 += db1 ** 2\n",
    "        self.accumulative_dw2 += dw2 ** 2\n",
    "        self.accumulative_db2 += db2 ** 2\n",
    "        \n",
    "        # Step 2\n",
    "        new_dw1 = dw1 / (np.sqrt(self.accumulative_dw1) + self.eps)\n",
    "        new_db1 = db1 / (np.sqrt(self.accumulative_db1) + self.eps)\n",
    "        new_dw2 = dw2 / (np.sqrt(self.accumulative_dw2) + self.eps)\n",
    "        new_db2 = db2 / (np.sqrt(self.accumulative_db2) + self.eps)\n",
    "\n",
    "        \n",
    "        # Step 3\n",
    "        self.model.gradient_descent(new_dw1, new_db1, new_dw2, new_db2, self.init_lr)\n",
    "\n",
    "        # End\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2af7c",
   "metadata": {},
   "source": [
    "### 2.4 Mini-batch Gradient Descent (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "316b60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def train_mbgd(model: 'MLP',\n",
    "               x_train: np.ndarray,\n",
    "               y_train: np.ndarray,\n",
    "               x_valid: np.ndarray,\n",
    "               y_valid: np.ndarray,\n",
    "               lr: float,\n",
    "               num_epoch: int,\n",
    "               batch_size: int,\n",
    "               print_every: int = 10) -> Tuple[dict[str, List], dict[str, List]]:\n",
    "    \"\"\" Training with Gradient Descent\n",
    "    Args:\n",
    "        model: The logistic regression model\n",
    "        x_train: Training feature, (n x d) matrix\n",
    "        y_train: Training label, (n, ) vector\n",
    "        x_valid: Validation feature, (n x d) matrix\n",
    "        y_valid: Validation label, (n, ) vector\n",
    "        lr: Learning rate\n",
    "        num_epoch: Number of training epochs\n",
    "        batch_size: Number of training samples in a batch\n",
    "        print_every: Print log every {print_every} epochs\n",
    "    Returns:\n",
    "        train_history: Log of training information. The format of training history is\n",
    "                       { 'loss': [] }\n",
    "                       It records the average loss of each epoch.\n",
    "        valid_history: Log of validation information. The format of training and validation history is\n",
    "                       {\n",
    "                           'loss': [],\n",
    "                           'accuracy': [],\n",
    "                           'precision': [],\n",
    "                           'recall': [],\n",
    "                           'f1': []\n",
    "                       }\n",
    "    \"\"\"\n",
    "    train_history = OrderedDict({'loss': []})\n",
    "    valid_history = OrderedDict({\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    })\n",
    "\n",
    "    def format_output(epoch, num_epoch, train_history, valid_history):\n",
    "        epoch_log = f'Epoch {epoch + 1} / {num_epoch}'\n",
    "        train_log = ' - '.join([f'train_{key}: {val[-1]:.4f}' for key, val in train_history.items()])\n",
    "        valid_log = ' - '.join([f'valid_{key}: {val[-1]:.4f}' for key, val in valid_history.items()])\n",
    "        log = f'{epoch_log}: {train_log} - {valid_log}'\n",
    "        return log\n",
    "\n",
    "    \n",
    "    optimizer = AdaGrad(init_lr=lr, model=model)\n",
    "\n",
    "    train_num_samples = len(x_train)\n",
    "    n_batch = train_num_samples // batch_size\n",
    "    y_valid = np.argmax(y_valid, axis=1)\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        # Start your code here (training)\n",
    "        #     Step 1. Model forward\n",
    "        \n",
    "        #     Step 2. Calculate loss\n",
    "        #     Step 3. Model backward\n",
    "        #     Step 4. Optimization with Adagrad\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat, h1 = model.forward(x_batch, return_hiddens=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = model.categorical_cross_entropy_loss(y_hat, y_batch)\n",
    "            epoch_loss += loss * len(x_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            dw1, db1, dw2, db2 = model.backward(x_batch, y_hat, y_batch, h1)\n",
    "\n",
    "            # Update parameters using AdaGrad\n",
    "            optimizer.update(dw1, db1, dw2, db2)\n",
    "\n",
    "        # End\n",
    "\n",
    "        valid_loss = 0.\n",
    "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        # Start your code here (validation)\n",
    "        #     Step 1. Predict\n",
    "        #     Step 2. Calculate loss\n",
    "        #     Step 3. Calculate metrics\n",
    "        y_hat_valid = model.forward(x_valid)\n",
    "        y_pred = np.argmax(y_hat_valid, axis=1)\n",
    "        valid_loss = model.categorical_cross_entropy_loss(y_pred, y_valid)  \n",
    "        \n",
    "        print(\"y_pred: \", y_pred.shape)\n",
    "        print(\"y_valid: \", y_valid)\n",
    "        accuracy, precision, recall, f1 = get_metrics(y_valid, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "        # Update histories\n",
    "        train_history['loss'].append(epoch_loss / len(x_train))\n",
    "        valid_history['loss'].append(valid_loss)\n",
    "        valid_history['accuracy'].append(accuracy)\n",
    "        valid_history['precision'].append(precision)\n",
    "        valid_history['recall'].append(recall)\n",
    "        valid_history['f1'].append(f1)\n",
    "\n",
    "        # End\n",
    "\n",
    "        train_history['loss'].append(epoch_loss / train_num_samples)\n",
    "        for vals, val in zip(valid_history.values(), [valid_loss, accuracy, precision, recall, f1]):\n",
    "            vals.append(val)\n",
    "        log = format_output(epoch, num_epoch, train_history, valid_history)\n",
    "        if epoch % print_every == 0 or epoch == num_epoch - 1:\n",
    "            print(log)\n",
    "        else:\n",
    "            print_line(log)\n",
    "\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c409d4",
   "metadata": {},
   "source": [
    "Run Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e65b35a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 1 / 100: train_loss: 0.9102 - valid_loss: 0.0000 - valid_accuracy: 0.5999 - valid_precision: 0.5999 - valid_recall: 0.5999 - valid_f1: 0.5999\n",
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)rain_loss: 0.7333 - valid_loss: 0.0000 - valid_accuracy: 0.8156 - valid_precision: 0.8156 - valid_recall: 0.8156 - valid_f1: 0.8156\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)rain_loss: 0.5706 - valid_loss: 0.0000 - valid_accuracy: 0.8502 - valid_precision: 0.8502 - valid_recall: 0.8502 - valid_f1: 0.8502\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)rain_loss: 0.4725 - valid_loss: 0.0000 - valid_accuracy: 0.8611 - valid_precision: 0.8611 - valid_recall: 0.8611 - valid_f1: 0.8611\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)rain_loss: 0.4103 - valid_loss: 0.0000 - valid_accuracy: 0.8643 - valid_precision: 0.8643 - valid_recall: 0.8643 - valid_f1: 0.8643\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)rain_loss: 0.3654 - valid_loss: 0.0000 - valid_accuracy: 0.8694 - valid_precision: 0.8694 - valid_recall: 0.8694 - valid_f1: 0.8694\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)rain_loss: 0.3300 - valid_loss: 0.0265 - valid_accuracy: 0.8784 - valid_precision: 0.8784 - valid_recall: 0.8784 - valid_f1: 0.8784\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)rain_loss: 0.3004 - valid_loss: 0.0796 - valid_accuracy: 0.8982 - valid_precision: 0.8982 - valid_recall: 0.8982 - valid_f1: 0.8982\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)rain_loss: 0.2750 - valid_loss: 0.1592 - valid_accuracy: 0.9014 - valid_precision: 0.9014 - valid_recall: 0.9014 - valid_f1: 0.9014\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.2528 - valid_loss: 0.2653 - valid_accuracy: 0.9104 - valid_precision: 0.9104 - valid_recall: 0.9104 - valid_f1: 0.9104\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 11 / 100: train_loss: 0.2334 - valid_loss: 0.2653 - valid_accuracy: 0.9187 - valid_precision: 0.9187 - valid_recall: 0.9187 - valid_f1: 0.9187\n",
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.2162 - valid_loss: 0.3184 - valid_accuracy: 0.9283 - valid_precision: 0.9283 - valid_recall: 0.9283 - valid_f1: 0.9283\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.2011 - valid_loss: 0.3184 - valid_accuracy: 0.9366 - valid_precision: 0.9366 - valid_recall: 0.9366 - valid_f1: 0.9366\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1877 - valid_loss: 0.3715 - valid_accuracy: 0.9385 - valid_precision: 0.9385 - valid_recall: 0.9385 - valid_f1: 0.9385\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1758 - valid_loss: 0.3847 - valid_accuracy: 0.9392 - valid_precision: 0.9392 - valid_recall: 0.9392 - valid_f1: 0.9392\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1653 - valid_loss: 0.4113 - valid_accuracy: 0.9430 - valid_precision: 0.9430 - valid_recall: 0.9430 - valid_f1: 0.9430\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1559 - valid_loss: 0.4113 - valid_accuracy: 0.9481 - valid_precision: 0.9481 - valid_recall: 0.9481 - valid_f1: 0.9481\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1476 - valid_loss: 0.4113 - valid_accuracy: 0.9481 - valid_precision: 0.9481 - valid_recall: 0.9481 - valid_f1: 0.9481\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1401 - valid_loss: 0.4378 - valid_accuracy: 0.9513 - valid_precision: 0.9513 - valid_recall: 0.9513 - valid_f1: 0.9513\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1333 - valid_loss: 0.4643 - valid_accuracy: 0.9539 - valid_precision: 0.9539 - valid_recall: 0.9539 - valid_f1: 0.9539\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 21 / 100: train_loss: 0.1272 - valid_loss: 0.4643 - valid_accuracy: 0.9565 - valid_precision: 0.9565 - valid_recall: 0.9565 - valid_f1: 0.9565\n",
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1217 - valid_loss: 0.4643 - valid_accuracy: 0.9577 - valid_precision: 0.9577 - valid_recall: 0.9577 - valid_f1: 0.9577\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1166 - valid_loss: 0.4643 - valid_accuracy: 0.9584 - valid_precision: 0.9584 - valid_recall: 0.9584 - valid_f1: 0.9584\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1121 - valid_loss: 0.4643 - valid_accuracy: 0.9584 - valid_precision: 0.9584 - valid_recall: 0.9584 - valid_f1: 0.9584\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1079 - valid_loss: 0.4643 - valid_accuracy: 0.9584 - valid_precision: 0.9584 - valid_recall: 0.9584 - valid_f1: 0.9584\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1040 - valid_loss: 0.4643 - valid_accuracy: 0.9590 - valid_precision: 0.9590 - valid_recall: 0.9590 - valid_f1: 0.9590\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.1004 - valid_loss: 0.4643 - valid_accuracy: 0.9590 - valid_precision: 0.9590 - valid_recall: 0.9590 - valid_f1: 0.9590\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0971 - valid_loss: 0.4643 - valid_accuracy: 0.9590 - valid_precision: 0.9590 - valid_recall: 0.9590 - valid_f1: 0.9590\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0941 - valid_loss: 0.4643 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0912 - valid_loss: 0.4643 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 31 / 100: train_loss: 0.0886 - valid_loss: 0.4909 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0861 - valid_loss: 0.4909 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0838 - valid_loss: 0.4909 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0817 - valid_loss: 0.5042 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0796 - valid_loss: 0.5042 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0777 - valid_loss: 0.5042 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0759 - valid_loss: 0.5042 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0742 - valid_loss: 0.5042 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:  (1562,)train_loss: 0.0726 - valid_loss: 0.5042 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0710 - valid_loss: 0.5042 - valid_accuracy: 0.9622 - valid_precision: 0.9622 - valid_recall: 0.9622 - valid_f1: 0.9622\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 41 / 100: train_loss: 0.0696 - valid_loss: 0.5042 - valid_accuracy: 0.9622 - valid_precision: 0.9622 - valid_recall: 0.9622 - valid_f1: 0.9622\n",
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0682 - valid_loss: 0.5307 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0669 - valid_loss: 0.5307 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0656 - valid_loss: 0.5307 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0645 - valid_loss: 0.5307 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0633 - valid_loss: 0.5307 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0622 - valid_loss: 0.5307 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0612 - valid_loss: 0.5307 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0602 - valid_loss: 0.5307 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0592 - valid_loss: 0.5307 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 51 / 100: train_loss: 0.0583 - valid_loss: 0.5307 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0574 - valid_loss: 0.5572 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0566 - valid_loss: 0.5572 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0557 - valid_loss: 0.5572 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0550 - valid_loss: 0.5838 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0542 - valid_loss: 0.5838 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0535 - valid_loss: 0.5838 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0528 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0521 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0514 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 61 / 100: train_loss: 0.0508 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0502 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0496 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0490 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0484 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0479 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0473 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0468 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0463 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0458 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 71 / 100: train_loss: 0.0454 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0449 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0445 - valid_loss: 0.5838 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0440 - valid_loss: 0.6103 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0436 - valid_loss: 0.6103 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0432 - valid_loss: 0.6103 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:  (1562,)train_loss: 0.0428 - valid_loss: 0.5970 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0424 - valid_loss: 0.5970 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0420 - valid_loss: 0.5970 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0416 - valid_loss: 0.5970 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 81 / 100: train_loss: 0.0413 - valid_loss: 0.5970 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0409 - valid_loss: 0.5970 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0406 - valid_loss: 0.5970 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0402 - valid_loss: 0.5970 - valid_accuracy: 0.9597 - valid_precision: 0.9597 - valid_recall: 0.9597 - valid_f1: 0.9597\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0399 - valid_loss: 0.5705 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0396 - valid_loss: 0.5705 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0393 - valid_loss: 0.5705 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0390 - valid_loss: 0.5705 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0387 - valid_loss: 0.5705 - valid_accuracy: 0.9603 - valid_precision: 0.9603 - valid_recall: 0.9603 - valid_f1: 0.9603\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0384 - valid_loss: 0.5440 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 91 / 100: train_loss: 0.0381 - valid_loss: 0.5440 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_pred:  (1562,)\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0378 - valid_loss: 0.5440 - valid_accuracy: 0.9609 - valid_precision: 0.9609 - valid_recall: 0.9609 - valid_f1: 0.9609\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0375 - valid_loss: 0.5174 - valid_accuracy: 0.9622 - valid_precision: 0.9622 - valid_recall: 0.9622 - valid_f1: 0.9622\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0372 - valid_loss: 0.5174 - valid_accuracy: 0.9622 - valid_precision: 0.9622 - valid_recall: 0.9622 - valid_f1: 0.9622\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0370 - valid_loss: 0.5174 - valid_accuracy: 0.9622 - valid_precision: 0.9622 - valid_recall: 0.9622 - valid_f1: 0.9622\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0367 - valid_loss: 0.5174 - valid_accuracy: 0.9622 - valid_precision: 0.9622 - valid_recall: 0.9622 - valid_f1: 0.9622\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0365 - valid_loss: 0.5174 - valid_accuracy: 0.9622 - valid_precision: 0.9622 - valid_recall: 0.9622 - valid_f1: 0.9622\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0362 - valid_loss: 0.5174 - valid_accuracy: 0.9622 - valid_precision: 0.9622 - valid_recall: 0.9622 - valid_f1: 0.9622\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "y_pred:  (1562,)train_loss: 0.0360 - valid_loss: 0.5174 - valid_accuracy: 0.9622 - valid_precision: 0.9622 - valid_recall: 0.9622 - valid_f1: 0.9622\n",
      "y_valid:  [2 2 1 ... 2 0 0]\n",
      "y_pred:  (1562,)\n",
      "y_true:  (1562,)\n",
      "Epoch 100 / 100: train_loss: 0.0357 - valid_loss: 0.5174 - valid_accuracy: 0.9622 - valid_precision: 0.9622 - valid_recall: 0.9622 - valid_f1: 0.9622\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "print_every = 10\n",
    "\n",
    "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "mbgd_train_history, mbgd_valid_history = train_mbgd(model_mbgd, x_train, y_train, x_valid, y_valid, lr, num_epoch, batch_size, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec42557",
   "metadata": {},
   "source": [
    "### 2.5 MLP using Tensorflow (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "888d3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.regularizers import l2 \n",
    "\n",
    "class MLPTF(Model):\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model using tensorflow.keras\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        # self.dense1 =\n",
    "        # self.dense2 =\n",
    "        # self.softmax = \n",
    "        self.dense1 = Dense(hidden_dim, activation='sigmoid', kernel_regularizer=l2(lambda_),\n",
    "                            input_shape=(feature_dim,))\n",
    "        self.dense2 = Dense(num_class, kernel_regularizer=l2(lambda_))\n",
    "\n",
    "        # End\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" Forward function of tf. It should be named 'call'\n",
    "        \n",
    "        Args:\n",
    "            x: (n x feature_dim) tensor\n",
    "        Returns:\n",
    "            y_hat: (n x num_class) tensor\n",
    "        \"\"\"\n",
    "        # Start your code here (Forward)\n",
    "        x = self.dense1(x)\n",
    "        y_hat = self.dense2(x)\n",
    "\n",
    "        # End\n",
    "        return tf.nn.softmax(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d183585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlptf_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             multiple                  1880832   \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1881219 (7.18 MB)\n",
      "Trainable params: 1881219 (7.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "np.random.seed(6666)\n",
    "tf.random.set_seed(6666)\n",
    "\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-1\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "\n",
    "model_tf = MLPTF(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "model_tf.build(input_shape=(None, vocab_size))\n",
    "model_tf.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=lr),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(num_classes=num_class, average='micro')])\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db646271",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 7s 44ms/step - loss: 1.0249 - categorical_accuracy: 0.5230 - f1_score: 0.5230 - val_loss: 1.0429 - val_categorical_accuracy: 0.5794 - val_f1_score: 0.5794\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.9532 - categorical_accuracy: 0.5559 - f1_score: 0.5559 - val_loss: 0.9313 - val_categorical_accuracy: 0.5794 - val_f1_score: 0.5794\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.9279 - categorical_accuracy: 0.5719 - f1_score: 0.5719 - val_loss: 0.9177 - val_categorical_accuracy: 0.5794 - val_f1_score: 0.5794\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.9111 - categorical_accuracy: 0.5802 - f1_score: 0.5802 - val_loss: 0.8821 - val_categorical_accuracy: 0.5813 - val_f1_score: 0.5813\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.8846 - categorical_accuracy: 0.5945 - f1_score: 0.5945 - val_loss: 0.8591 - val_categorical_accuracy: 0.5839 - val_f1_score: 0.5839\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.8558 - categorical_accuracy: 0.6277 - f1_score: 0.6277 - val_loss: 0.8332 - val_categorical_accuracy: 0.6344 - val_f1_score: 0.6344\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.8201 - categorical_accuracy: 0.6726 - f1_score: 0.6726 - val_loss: 0.7880 - val_categorical_accuracy: 0.6607 - val_f1_score: 0.6607\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 4s 35ms/step - loss: 0.7805 - categorical_accuracy: 0.7087 - f1_score: 0.7087 - val_loss: 0.7513 - val_categorical_accuracy: 0.6895 - val_f1_score: 0.6895\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 4s 33ms/step - loss: 0.7406 - categorical_accuracy: 0.7335 - f1_score: 0.7335 - val_loss: 0.7148 - val_categorical_accuracy: 0.7260 - val_f1_score: 0.7260\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 3s 31ms/step - loss: 0.6986 - categorical_accuracy: 0.7539 - f1_score: 0.7539 - val_loss: 0.6748 - val_categorical_accuracy: 0.7785 - val_f1_score: 0.7785\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.6631 - categorical_accuracy: 0.7703 - f1_score: 0.7703 - val_loss: 0.6411 - val_categorical_accuracy: 0.7907 - val_f1_score: 0.7907\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 3s 24ms/step - loss: 0.6291 - categorical_accuracy: 0.7813 - f1_score: 0.7813 - val_loss: 0.6113 - val_categorical_accuracy: 0.7913 - val_f1_score: 0.7913\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 3s 23ms/step - loss: 0.5993 - categorical_accuracy: 0.7919 - f1_score: 0.7919 - val_loss: 0.5844 - val_categorical_accuracy: 0.7868 - val_f1_score: 0.7868\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.5714 - categorical_accuracy: 0.8012 - f1_score: 0.8012 - val_loss: 0.5593 - val_categorical_accuracy: 0.8060 - val_f1_score: 0.8060\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.5476 - categorical_accuracy: 0.8070 - f1_score: 0.8070 - val_loss: 0.5395 - val_categorical_accuracy: 0.7945 - val_f1_score: 0.7945\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 3s 24ms/step - loss: 0.5257 - categorical_accuracy: 0.8128 - f1_score: 0.8128 - val_loss: 0.5205 - val_categorical_accuracy: 0.8239 - val_f1_score: 0.8239\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.5050 - categorical_accuracy: 0.8191 - f1_score: 0.8191 - val_loss: 0.4932 - val_categorical_accuracy: 0.8227 - val_f1_score: 0.8227\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 3s 23ms/step - loss: 0.4864 - categorical_accuracy: 0.8241 - f1_score: 0.8241 - val_loss: 0.4873 - val_categorical_accuracy: 0.8457 - val_f1_score: 0.8457\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.4686 - categorical_accuracy: 0.8297 - f1_score: 0.8297 - val_loss: 0.4622 - val_categorical_accuracy: 0.8227 - val_f1_score: 0.8227\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.4522 - categorical_accuracy: 0.8329 - f1_score: 0.8329 - val_loss: 0.4443 - val_categorical_accuracy: 0.8367 - val_f1_score: 0.8367\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.4363 - categorical_accuracy: 0.8370 - f1_score: 0.8370 - val_loss: 0.4306 - val_categorical_accuracy: 0.8425 - val_f1_score: 0.8425\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.4214 - categorical_accuracy: 0.8418 - f1_score: 0.8418 - val_loss: 0.4157 - val_categorical_accuracy: 0.8470 - val_f1_score: 0.8470\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.4079 - categorical_accuracy: 0.8473 - f1_score: 0.8473 - val_loss: 0.4023 - val_categorical_accuracy: 0.8515 - val_f1_score: 0.8515\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.3951 - categorical_accuracy: 0.8525 - f1_score: 0.8525 - val_loss: 0.3910 - val_categorical_accuracy: 0.8540 - val_f1_score: 0.8540\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 3s 25ms/step - loss: 0.3825 - categorical_accuracy: 0.8568 - f1_score: 0.8568 - val_loss: 0.3798 - val_categorical_accuracy: 0.8643 - val_f1_score: 0.8643\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 3s 24ms/step - loss: 0.3707 - categorical_accuracy: 0.8600 - f1_score: 0.8600 - val_loss: 0.3694 - val_categorical_accuracy: 0.8604 - val_f1_score: 0.8604\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.3590 - categorical_accuracy: 0.8666 - f1_score: 0.8666 - val_loss: 0.3580 - val_categorical_accuracy: 0.8579 - val_f1_score: 0.8579\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.3485 - categorical_accuracy: 0.8683 - f1_score: 0.8683 - val_loss: 0.3475 - val_categorical_accuracy: 0.8707 - val_f1_score: 0.8707\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.3380 - categorical_accuracy: 0.8702 - f1_score: 0.8702 - val_loss: 0.3384 - val_categorical_accuracy: 0.8656 - val_f1_score: 0.8656\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 8s 67ms/step - loss: 0.3284 - categorical_accuracy: 0.8745 - f1_score: 0.8745 - val_loss: 0.3299 - val_categorical_accuracy: 0.8649 - val_f1_score: 0.8649\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.3189 - categorical_accuracy: 0.8779 - f1_score: 0.8779 - val_loss: 0.3206 - val_categorical_accuracy: 0.8771 - val_f1_score: 0.8771\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.3094 - categorical_accuracy: 0.8824 - f1_score: 0.8824 - val_loss: 0.3128 - val_categorical_accuracy: 0.8803 - val_f1_score: 0.8803\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.3018 - categorical_accuracy: 0.8848 - f1_score: 0.8848 - val_loss: 0.3048 - val_categorical_accuracy: 0.8803 - val_f1_score: 0.8803\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 2s 23ms/step - loss: 0.2934 - categorical_accuracy: 0.8879 - f1_score: 0.8879 - val_loss: 0.2997 - val_categorical_accuracy: 0.8854 - val_f1_score: 0.8854\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 3s 26ms/step - loss: 0.2851 - categorical_accuracy: 0.8912 - f1_score: 0.8912 - val_loss: 0.2920 - val_categorical_accuracy: 0.8873 - val_f1_score: 0.8873\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.2782 - categorical_accuracy: 0.8956 - f1_score: 0.8956 - val_loss: 0.2852 - val_categorical_accuracy: 0.8880 - val_f1_score: 0.8880\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.2713 - categorical_accuracy: 0.8966 - f1_score: 0.8966 - val_loss: 0.2803 - val_categorical_accuracy: 0.8988 - val_f1_score: 0.8988\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.2647 - categorical_accuracy: 0.9016 - f1_score: 0.9016 - val_loss: 0.2723 - val_categorical_accuracy: 0.8924 - val_f1_score: 0.8924\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.2577 - categorical_accuracy: 0.9035 - f1_score: 0.9035 - val_loss: 0.2677 - val_categorical_accuracy: 0.8976 - val_f1_score: 0.8976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.2515 - categorical_accuracy: 0.9069 - f1_score: 0.9069 - val_loss: 0.2610 - val_categorical_accuracy: 0.8988 - val_f1_score: 0.8988\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.2453 - categorical_accuracy: 0.9096 - f1_score: 0.9096 - val_loss: 0.2551 - val_categorical_accuracy: 0.8976 - val_f1_score: 0.8976\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.2394 - categorical_accuracy: 0.9122 - f1_score: 0.9122 - val_loss: 0.2583 - val_categorical_accuracy: 0.9168 - val_f1_score: 0.9168\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.2343 - categorical_accuracy: 0.9165 - f1_score: 0.9165 - val_loss: 0.2452 - val_categorical_accuracy: 0.9020 - val_f1_score: 0.9020\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.2288 - categorical_accuracy: 0.9174 - f1_score: 0.9174 - val_loss: 0.2412 - val_categorical_accuracy: 0.9085 - val_f1_score: 0.9085\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 8s 69ms/step - loss: 0.2236 - categorical_accuracy: 0.9208 - f1_score: 0.9208 - val_loss: 0.2373 - val_categorical_accuracy: 0.9104 - val_f1_score: 0.9104\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.2190 - categorical_accuracy: 0.9215 - f1_score: 0.9215 - val_loss: 0.2336 - val_categorical_accuracy: 0.9155 - val_f1_score: 0.9155\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.2143 - categorical_accuracy: 0.9251 - f1_score: 0.9251 - val_loss: 0.2281 - val_categorical_accuracy: 0.9142 - val_f1_score: 0.9142\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.2099 - categorical_accuracy: 0.9259 - f1_score: 0.9259 - val_loss: 0.2247 - val_categorical_accuracy: 0.9149 - val_f1_score: 0.9149\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.2053 - categorical_accuracy: 0.9301 - f1_score: 0.9301 - val_loss: 0.2224 - val_categorical_accuracy: 0.9200 - val_f1_score: 0.9200\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 7s 67ms/step - loss: 0.2011 - categorical_accuracy: 0.9313 - f1_score: 0.9313 - val_loss: 0.2176 - val_categorical_accuracy: 0.9155 - val_f1_score: 0.9155\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.1974 - categorical_accuracy: 0.9333 - f1_score: 0.9333 - val_loss: 0.2140 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1934 - categorical_accuracy: 0.9359 - f1_score: 0.9359 - val_loss: 0.2110 - val_categorical_accuracy: 0.9104 - val_f1_score: 0.9104\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 5s 42ms/step - loss: 0.1895 - categorical_accuracy: 0.9370 - f1_score: 0.9370 - val_loss: 0.2066 - val_categorical_accuracy: 0.9174 - val_f1_score: 0.9174\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 5s 47ms/step - loss: 0.1860 - categorical_accuracy: 0.9384 - f1_score: 0.9384 - val_loss: 0.2067 - val_categorical_accuracy: 0.9341 - val_f1_score: 0.9341\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1825 - categorical_accuracy: 0.9408 - f1_score: 0.9408 - val_loss: 0.2009 - val_categorical_accuracy: 0.9225 - val_f1_score: 0.9225\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1795 - categorical_accuracy: 0.9423 - f1_score: 0.9423 - val_loss: 0.1974 - val_categorical_accuracy: 0.9219 - val_f1_score: 0.9219\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1759 - categorical_accuracy: 0.9436 - f1_score: 0.9436 - val_loss: 0.1951 - val_categorical_accuracy: 0.9296 - val_f1_score: 0.9296\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.1729 - categorical_accuracy: 0.9446 - f1_score: 0.9446 - val_loss: 0.1928 - val_categorical_accuracy: 0.9219 - val_f1_score: 0.9219\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1700 - categorical_accuracy: 0.9478 - f1_score: 0.9478 - val_loss: 0.1910 - val_categorical_accuracy: 0.9341 - val_f1_score: 0.9341\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1668 - categorical_accuracy: 0.9478 - f1_score: 0.9478 - val_loss: 0.1893 - val_categorical_accuracy: 0.9398 - val_f1_score: 0.9398\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1639 - categorical_accuracy: 0.9493 - f1_score: 0.9493 - val_loss: 0.1852 - val_categorical_accuracy: 0.9334 - val_f1_score: 0.9334\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1614 - categorical_accuracy: 0.9508 - f1_score: 0.9508 - val_loss: 0.1832 - val_categorical_accuracy: 0.9302 - val_f1_score: 0.9302\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1587 - categorical_accuracy: 0.9523 - f1_score: 0.9523 - val_loss: 0.1804 - val_categorical_accuracy: 0.9341 - val_f1_score: 0.9341\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.1563 - categorical_accuracy: 0.9526 - f1_score: 0.9526 - val_loss: 0.1795 - val_categorical_accuracy: 0.9437 - val_f1_score: 0.9437\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.1538 - categorical_accuracy: 0.9548 - f1_score: 0.9548 - val_loss: 0.1761 - val_categorical_accuracy: 0.9347 - val_f1_score: 0.9347\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 8s 71ms/step - loss: 0.1514 - categorical_accuracy: 0.9551 - f1_score: 0.9551 - val_loss: 0.1780 - val_categorical_accuracy: 0.9475 - val_f1_score: 0.9475\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.1491 - categorical_accuracy: 0.9561 - f1_score: 0.9561 - val_loss: 0.1731 - val_categorical_accuracy: 0.9379 - val_f1_score: 0.9379\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1467 - categorical_accuracy: 0.9562 - f1_score: 0.9562 - val_loss: 0.1706 - val_categorical_accuracy: 0.9366 - val_f1_score: 0.9366\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1448 - categorical_accuracy: 0.9579 - f1_score: 0.9579 - val_loss: 0.1701 - val_categorical_accuracy: 0.9475 - val_f1_score: 0.9475\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.1424 - categorical_accuracy: 0.9603 - f1_score: 0.9603 - val_loss: 0.1671 - val_categorical_accuracy: 0.9411 - val_f1_score: 0.9411\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 7s 68ms/step - loss: 0.1406 - categorical_accuracy: 0.9599 - f1_score: 0.9599 - val_loss: 0.1661 - val_categorical_accuracy: 0.9456 - val_f1_score: 0.9456\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 3s 24ms/step - loss: 0.1385 - categorical_accuracy: 0.9616 - f1_score: 0.9616 - val_loss: 0.1640 - val_categorical_accuracy: 0.9456 - val_f1_score: 0.9456\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1364 - categorical_accuracy: 0.9613 - f1_score: 0.9613 - val_loss: 0.1633 - val_categorical_accuracy: 0.9488 - val_f1_score: 0.9488\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1344 - categorical_accuracy: 0.9632 - f1_score: 0.9632 - val_loss: 0.1606 - val_categorical_accuracy: 0.9481 - val_f1_score: 0.9481\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1328 - categorical_accuracy: 0.9631 - f1_score: 0.9631 - val_loss: 0.1595 - val_categorical_accuracy: 0.9475 - val_f1_score: 0.9475\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1308 - categorical_accuracy: 0.9645 - f1_score: 0.9645 - val_loss: 0.1588 - val_categorical_accuracy: 0.9494 - val_f1_score: 0.9494\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1292 - categorical_accuracy: 0.9647 - f1_score: 0.9647 - val_loss: 0.1576 - val_categorical_accuracy: 0.9481 - val_f1_score: 0.9481\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - 2s 22ms/step - loss: 0.1276 - categorical_accuracy: 0.9651 - f1_score: 0.9651 - val_loss: 0.1557 - val_categorical_accuracy: 0.9507 - val_f1_score: 0.9507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "110/110 [==============================] - 7s 66ms/step - loss: 0.1259 - categorical_accuracy: 0.9656 - f1_score: 0.9656 - val_loss: 0.1540 - val_categorical_accuracy: 0.9462 - val_f1_score: 0.9462\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1243 - categorical_accuracy: 0.9656 - f1_score: 0.9656 - val_loss: 0.1533 - val_categorical_accuracy: 0.9545 - val_f1_score: 0.9545\n",
      "Epoch 81/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1227 - categorical_accuracy: 0.9674 - f1_score: 0.9674 - val_loss: 0.1514 - val_categorical_accuracy: 0.9507 - val_f1_score: 0.9507\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1212 - categorical_accuracy: 0.9672 - f1_score: 0.9672 - val_loss: 0.1502 - val_categorical_accuracy: 0.9488 - val_f1_score: 0.9488\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1197 - categorical_accuracy: 0.9679 - f1_score: 0.9679 - val_loss: 0.1504 - val_categorical_accuracy: 0.9501 - val_f1_score: 0.9501\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1182 - categorical_accuracy: 0.9679 - f1_score: 0.9679 - val_loss: 0.1489 - val_categorical_accuracy: 0.9501 - val_f1_score: 0.9501\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 7s 67ms/step - loss: 0.1168 - categorical_accuracy: 0.9691 - f1_score: 0.9691 - val_loss: 0.1466 - val_categorical_accuracy: 0.9520 - val_f1_score: 0.9520\n",
      "Epoch 86/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1156 - categorical_accuracy: 0.9689 - f1_score: 0.9689 - val_loss: 0.1459 - val_categorical_accuracy: 0.9552 - val_f1_score: 0.9552\n",
      "Epoch 87/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1141 - categorical_accuracy: 0.9705 - f1_score: 0.9705 - val_loss: 0.1453 - val_categorical_accuracy: 0.9545 - val_f1_score: 0.9545\n",
      "Epoch 88/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1126 - categorical_accuracy: 0.9701 - f1_score: 0.9701 - val_loss: 0.1443 - val_categorical_accuracy: 0.9513 - val_f1_score: 0.9513\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - 3s 23ms/step - loss: 0.1113 - categorical_accuracy: 0.9708 - f1_score: 0.9708 - val_loss: 0.1440 - val_categorical_accuracy: 0.9488 - val_f1_score: 0.9488\n",
      "Epoch 90/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1102 - categorical_accuracy: 0.9708 - f1_score: 0.9708 - val_loss: 0.1424 - val_categorical_accuracy: 0.9539 - val_f1_score: 0.9539\n",
      "Epoch 91/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1090 - categorical_accuracy: 0.9723 - f1_score: 0.9723 - val_loss: 0.1416 - val_categorical_accuracy: 0.9539 - val_f1_score: 0.9539\n",
      "Epoch 92/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1079 - categorical_accuracy: 0.9720 - f1_score: 0.9720 - val_loss: 0.1406 - val_categorical_accuracy: 0.9558 - val_f1_score: 0.9558\n",
      "Epoch 93/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1067 - categorical_accuracy: 0.9726 - f1_score: 0.9726 - val_loss: 0.1393 - val_categorical_accuracy: 0.9565 - val_f1_score: 0.9565\n",
      "Epoch 94/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1054 - categorical_accuracy: 0.9735 - f1_score: 0.9735 - val_loss: 0.1392 - val_categorical_accuracy: 0.9565 - val_f1_score: 0.9565\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1043 - categorical_accuracy: 0.9732 - f1_score: 0.9732 - val_loss: 0.1388 - val_categorical_accuracy: 0.9558 - val_f1_score: 0.9558\n",
      "Epoch 96/100\n",
      "110/110 [==============================] - 2s 20ms/step - loss: 0.1033 - categorical_accuracy: 0.9743 - f1_score: 0.9743 - val_loss: 0.1369 - val_categorical_accuracy: 0.9545 - val_f1_score: 0.9545\n",
      "Epoch 97/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1020 - categorical_accuracy: 0.9741 - f1_score: 0.9741 - val_loss: 0.1361 - val_categorical_accuracy: 0.9558 - val_f1_score: 0.9558\n",
      "Epoch 98/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1010 - categorical_accuracy: 0.9746 - f1_score: 0.9746 - val_loss: 0.1353 - val_categorical_accuracy: 0.9565 - val_f1_score: 0.9565\n",
      "Epoch 99/100\n",
      "110/110 [==============================] - 2s 21ms/step - loss: 0.1000 - categorical_accuracy: 0.9753 - f1_score: 0.9753 - val_loss: 0.1348 - val_categorical_accuracy: 0.9571 - val_f1_score: 0.9571\n",
      "Epoch 100/100\n",
      "110/110 [==============================] - 2s 23ms/step - loss: 0.0991 - categorical_accuracy: 0.9748 - f1_score: 0.9748 - val_loss: 0.1344 - val_categorical_accuracy: 0.9577 - val_f1_score: 0.9577\n"
     ]
    }
   ],
   "source": [
    "tf_history = model_tf.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=batch_size, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8035c0",
   "metadata": {},
   "source": [
    "#### Evaluation with Tensroflow\n",
    "You are required to report the loss, accuracy, precision, recall, and f1 on test set and plot the the curve of them for both SGD and Mini-batch GD on train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "546dce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:  (3907, 3)\n",
      "y_true:  (3907, 3)\n",
      "Mini-batch GD: (0.9721866734920228, 0.9582800102380343, 0.9582800102380343, 0.9582800102380343)\n",
      "123/123 [==============================] - 0s 3ms/step - loss: 1.1049 - categorical_accuracy: 0.5595 - f1_score: 0.5595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1049187183380127, 0.5595085620880127, 0.5595085620880127]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the metrics for test set and fill in the table below\n",
    "y_hat = model_mbgd.forward(x_test)\n",
    "y_pred = model_mbgd.predict(y_hat)\n",
    "print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
    "model_tf.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e13bb3",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics on Test set\n",
    "Fill this table with the result you just printed (double click this cell to edit)\n",
    "|     Optimizer                     | Accuracy    | F1 Score    |\n",
    "|:---------------------------------:|-------------|-------------|\n",
    "|      **Your Implementation**      |     .9622   |     .9622   |\n",
    "| **Tensorflow**                    |     .9577   |     .9577   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f40e9",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the training loss curve for Your implementation and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4a23bb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.plot(mbgd_train_history['loss'], label='My MLP')\n",
    "plt.plot(tf_history.history['loss'], label='TF MLP')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a769ce6",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the validation metrics curve for SGD and Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85904d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax = axes[0]\n",
    "ax.plot(mbgd_valid_history['accuracy'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_categorical_accuracy'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation Accuracy')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(mbgd_valid_history['f1'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_f1_score'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation F1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab356956",
   "metadata": {},
   "source": [
    "# 3. Conclusion (5 Points)\n",
    "\n",
    "Provide an analysis for all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909f1ff",
   "metadata": {},
   "source": [
    "Answer: In evaluating the performance of my machine learning models, I observed some fascinating outcomes. My implementation of the MLP, which utilizes Mini-batch Gradient Descent, significantly outperformed the TensorFlow model in terms of accuracy and F1 score on the test dataset. Notably, it achieved an accuracy and F1 score around 0.9622, whereas TensorFlow's version only managed about 0.9577. This stark contrast highlights the effectiveness of my model. Moreover, I noticed that my model reached its peak performance approximately 50 epochs before TensorFlow's, suggesting greater training efficiency. The training loss curves further supported this, as they showed a smoother and quicker decrease in loss for my model, implying better optimization. The validation metrics, particularly accuracy and F1 score, also mirrored these trends, indicating that my model was not only learning efficiently but also generalizing well to new data. This experience has been incredibly enlightening, and it underscores the importance of fine-tuning model architecture and hyperparameters to achieve optimal performance. It also prompts me to further explore why TensorFlow's model didn't perform as well, possibly by reviewing its configuration and training approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
